Notes on: "real-time collision detection" by Christer Ericson

explicit representation,
objects represented by polygons, through their points, edges and faces

implicit representation,
geometrical objects such as spheres, cones, cylinders, tori (pl. torus), etc. which
can't be explicitly represented but implicitly through a mathematical expression:

implicit representation are usually represented as a mapping from 3d-space to 1d-real-numbers
like so:

[first, new, cool, math thing!]

f:R^3 -> R, where points given:
f(x,y,z) // our R^3
will evaluate into three regions zero, positive and negative:

f(x,y,z) = 0 // boundary of the object
f(x,y,z) > 0 // exterior of the object
f(x,y,z) < 0 // interior of the object

awesome!

Example for a sphere:
x^2 + y^2 + z^2 <= r^2    // works!

This is like a double Pythagorean theorem on the left side the resulting
distance is, if equal r^2, a point _on_ the sphere

(defun sphere-collision-p (x y z sphere-radius)
  "Returns true if the point P(x,y,z) lies within or touches the sphere of radius:
   sphere-radius."
  (<= (apply #'+
	     (mapcar #'(lambda (x) (expt x 2))
		     (list x y z)))
      (expt sphere-radius 2)))

Through understanding that this is just a double-pythagoras we can formulate a collision
test for a 2d-circle easily:
x^2 + y^2 <= r^2  (awesome number two!)

An object boundary defined in such a way is called an /implicit surface/!

Advantages:
- quick rejection culling (is it in the interior = CULL it out!)
- fast intersection test: well simply feed a point into the f(x,y,z) function and you know
  if it is inside or touching the boundary: f(x,y,z) =< 0!

half-space,
we cut space in two along an infinite plane. The resulting two spaces left and right of
the plane are infinitely large but still just half-spaces of a bigger infinity!

convex objects (polygon),
any two points within this object can connect in a straight line without leaving the region of
the object. We need this word because collision tests are much easier with such objects!
Foreshadowing: we will probably try to break concave polygons into smaller convex objects?

concave polygon object,
opposite of convex object. Contains an interior angle greater than 180-degrees.

half-space intersection representation (convex objects only),
another way to represent polygonal objects, here we cut away some space with planes and use
the resulting half-space intersections to represent an object. An example would be to represent
a rectangle in 2d by using two x-axis values to cut away the two sides - resulting in an
infinitely long rectangle along the y axis- then provide two y-axis to encompass a finitely
big rectangle! (see texatl.cl:space for this kind of rectangle representation!)
This is reminiscent of the aabb test!

--------------------------------------------------------------------------------
We won't usually use the same geometry for collision as for rendering
Pros:
- it is usually too complex, and collision test don't need to be that precise
- the usual triangle representation of objects are harder to query for collision
  then other shapes like spheres, for example
- rendering data may be organized in quite a different way than is useful for
  collision. Collision tests probably want spatial hierarchical relationships
  whereas rendering data will create topologies for adjacent vertices at best
  and material attributes, color and texture coordinates at worst (useless for
  collision tests!)
- organizing for collision tests makes the data smaller hence more efficient to
  use and coherent in memory and useful for further extension and tests tailored
  for collision
- sometimes you want to explicitly differ, for example the plane representing
  snow might be intentionally lowered in the proxy geometry to allow knee-deep
  movement in the snow. Instead for exclusion tests, just use a different geometry
  to begin with.
- even if rendering data is not needed because an object is not visible it is
  still needed for collision. Just imagine walking behind a wall, where the
  camera sees the wall it will usually cull out everything behind it, you should
  however still follow the law's of physics even when nobody is around to see it!

Cons:
- Data duplication. Could be solved by inferring the proxy geometry from the rendering
  geometry (through linearization caching)!!
- time to build the tools that generate the geometry or more time for the designer
  to create his models
- visual consistency may suffer: objects may float above others, or fully intersect
  (surely you seen the animations which simply move through anything when parts
   of it move outside the hitbox)
- interdependency problems: if the rendering geometry changes, the collision geometry
  might also have to change, and how is this process automated. Which of the two
  geometry comes first?

we use, hence, another geometry instead.. as a proxy:

proxy geometry,
the collision test geometry for objects. This usually reduces the complex rendering
geometry by encapsulating a object of interest into a:

bounding volume,
a simpler geometrical object like a box or a sphere representing the object in a
simpler way than the actual rendering geometry for the sake of a simpler easier
collision test!

--------------------------------------------------------------------------------
Types of queries:

interference detection or intersection testing,
yes/no did the objects collide, do they intersect?
Easy to implement, commonly used

intersection finding - WHERE did they intersect?
One contact point may sometimes suffice other times, as in rigid body simulations,
we need to detect a whole set of contact points /the contact manifold/!
Calculating the contact manifold may be difficult, and approximations may be used -
a common practice in games.

collision properties,
special collision queries, taking surface properties into account like slipperiness of a
road and climbability of a wall

penetration depth,
how deep are two objects overlapping. The problem here is what start and end point to
use to measure this. A solution to this can be the shortest movement vector needed
to separate the already interpenetrated objects. Its computation is also a difficult
problem.

/separation distance/, has a beautiful definition:
"The separation distance between two disjoint objects A and B is defined as _the minimum
of the distances between points in A and points in B_"!
Needed the predict the time (WHEN) two objects will collide. Also it poses another
problem of finding the

closest point,
in both the disjoint object A and B to each other. There can be infinite closest points!

ETA, /estimated time of arrival/, or, more dramatic, TOI, /time of impact/

--------------------------------------------------------------------------------
pairwise collision tests for n objects, take O(n^2) time - every objects with
every other object n x n!
But we can use a divide and conquer approach:

broad phase or n-<body> processing,
use a rule to decide which objects /may/ collide. For example divide a scene into
square regions, and whenever an object is inside a given region it is mapped to
some square-region data structure. This data structure can then exclude all the
objects that are sole inhabitants of a region from any collision test. The
other objects under go the

narrow phase (pair processing),
perform all the pairwise test on the unconquered parts (those objects
sharing the same square region!).

--------------------------------------------------------------------------------
Sequential vs simultaneous motion,
for both we break down movement to the unit of _time step_(!). Simultaneous motion
is more realistic but more expensive to compute:
(1) determine earliest time of contact
(2) move all objects for the extend of the no-contact-time
(3) resolve the collision
(4) repeat for the extend of the time step
A problem occurs when the object moves along a surface, then a collision resolution
would take place all the time and the simulation would only advance by a small
fraction.

While broad phase grouping can help divide and conquer the problem.  An alternative
simultaneous motion solution: advance the objects by a fixed small time step, when an
interpenetration of objects during a time step occurs restore the state before the
time step. This is also expensive.


The accuracy of simultaneous motion should be reserved for rigid-body simulations.
For most program sequential motion suffices (including games).

Sequential motion accuracy problems:
(1) Two objects during motion, in a sequential approach one object might move past the
collision point, then the other object also moves past it -> no collision occurs. (2) One
object is just behind another object, when it moves first it will catch up but if both
move at the same speed this catching up wouldn't make sense.  Both these issues _can be
solved by just making the time step smaller_!
In games the small time step is easily provided by the _high frame rate_(!)

sequential motion benefits include easy penetration resolution: just undo the single
motion during the small time step, this is to be contrasted by the simultaneous where we
have to undo motion during a time step for _all_ the objects (possibly minus some
broad phase excluded ones)!

--------------------------------------------------------------------------------
Discrete vs continuous motion

Discrete motion - static collision detection,
Collision detection is measured at a specific time, and the objects are treated as if
they're frozen in said time (This is much cheaper than dynamic collision detection below)
The object effectively teleports from time step to time step, the collision test is hence
less precise. On the note of teleportation: the time step teleportation distance has to be
smaller than the objects spatial extend or else the object might teleport through other
objects with no collision being detected, this is aptly called /tunneling/!!!

Excursion on tunneling:
To have more fun memorizing this, in quantum mechanics teleportation is an everyday thing
performed by particles all the time, called quantum tunneling. According to QM electrons
perform quantum leaps to move between energy states in atom, as the wave(wave-particle
duality) travels around the nuclei of the atom it reinforces itself on each iteration
thereby maintaining its energy level.

Another exciting thing: scientist were able to quantum tunnel a particle over the distance
of 9 meters. But this doesn't mean teleportation of humans is gonna any time happen soon,
when prompted for this an expert said that they had to get the particle in a particular
state before performing this, and the particles in human bodies just can't be manipulated
into such a state anytime soon.
/Excursion

dynamic collision detection,
we detect the collision by considering the continuous motion - meaning over a given time
interval or between start and end of a time step. This is both more accurate and more
expensive. More accurate because we can usually tell the exact time and point(s) of first
contact.

swept volume,
in continuous motion, this is what we refer to the space carved out by an object as it
moves for the extend of a single time step! This can be used for the broad phase: only if
the two objects' swept volumes do intersect do they become member of the same narrow phase
group. Of course even then collision is not guaranteed (should be easy to imagine
why). But imagine an object moving in a non-trivial path during the time step, the swept
volume can then be hard to compute and work with! Fortunately such accuracy is rarely
needed, and can be reduced for simplification to a linear motion.

speedbox,
in games the swept volume is usually simplified to an elongated box covering the objects
full range of motion (or some similarly simple proxy object).


relative motion,
what-is (high-level explanation):
subtracting two motion vectors hence one being stationary and the other moving towards it
for simpler intersection finding of the swept volumes. Consider v1 v2, for v1-v2, v2 is
the stationary "observer" where v1-v2 is the vector that moves towards it from v1 while,
again, v2 can't move. This explanation though is enough to work abstractly I want to
understand this at a visceral - intuitive level.
how-to (how to implement it explanation): (TODO: test this)
given two motion vectors v1 and v2 it is virtually always better to consider the relative
motion:
vp1 vp2 are position vectors, and v1 v2 are these vectors' motion (direction) vectors
the relative motion is the subtraction: v1-v2
The observer is the right-hand vector: vp2
it is immobile as vp1+(v1-v2) moves towards it (as far as the time step allows).
This is better because we now only have the swept volume carved out by vp1+(v1-v2)

Optimization,
is mainly achieved by the broad phase, putting objects into regions or region hierarchies that
may even be recursively extended when more objects govern a certain region (implementation
example: quadtree).

Robustness,
two problems: floating point imprecision and geometry nonsense through transformation. One
can give rise to the other, for example: space inverting on itself, through interpolation
off-by-one bug turning into extrapolation; or transformation order is violated and rotation
scaling distorts the geometry, or transforming normals on a non-uniform object (transpose
(inverse norm-matrix)), or systems prone to gimbal lock may have unexpected orientation at
certain stages of interpolation.  The collision system must be prepare and avoid those for..
robustness!


------------------------------------------------------------------------------
Math and geometry primer


transpose matrix of A: A^t

/skew symmetric/ or /antisemitic/ matrix,
when A^t = -A. For example rotation about the x-axis, then the antisemitic matrix
would be a 180-degree turn around the x-axis

;; 90-degrees              ;; 180-degree of 90-degrees turn before
#<[1.0, 0.0, 0.0, 0.0]     #<[1.0, 0.0, 0.0, 0.0]
  [0.0, 0.0, -1.0, 0.0]	     [0.0, 0.0, 1.0, 0.0]
  [0.0, 1.0, 0.0, 0.0]	     [0.0, -1.0, 0.0, 0.0]
  [0.0, 0.0, 0.0, 1.0]>	     [0.0, 0.0, 0.0, 1.0]>

matrix addition,
pairwise addition of corresponding positions. Given matrix A=[a.ij] and B=[b.ij]
then the sum matrix C=[c.ij] = A+B is defined as the pairwise addition of elements:
c.ij = a.ij + b.ij


lower triangular matrix,
all the entries of the matrix above the main diagonal are zero

upper triangular matrix,
opposite of lower triangular matrix

row/column vector or row/column matrix,
an m x n (m by n) matrix is comprised of smaller row/column matrices that make
up the eponymous rows/columns of the matrix. Also called the row/column vector.

singular (or noninvertible),
matrices that do _not_ have an inverse. For example: any matrix that has an all-zero
row.
Or more formally: if the determinant of matrix is 0, then the matrix is noninvertible.


Algebraic identities involving matrices,
given scalar r and s and matrices A, B and C (of appropriate sizes required to perform
the operations) the following identities hold:

s(A+B) = s*A + s*B     ;; i.e. scalars can be factored out
A(s+r) = r*A + s*A     ;; matrices too

scalar*matrix multiplications are associative
r(s*A) = s(r*A) = (r*s)A

A(B+C) = AB + AC

(s*A)*B = s(A*B) = A(s*B)  ;; tricky

transposing,
(A+B)^t = A^t + B^t  ;; is distributive:
(s*A)^t = s*A^t      ;; doesn't mind scalars
(A*B)^t = B^t*A^t    ;; switches the order when dissolving parens! tricky!!


--------------------------------------------------------------------------------
Determinants det(A) or |A|
it is a _single number_ associated with the matrix and it tells us if the systems of
linear equations, that matrix is comprised of, is _solvable_. It is solvable when the
values is det(A) != 0. One of the things we want to solve is if the matrix is
_invertible_.

We will mainly need to calculate the determinant of 2 x 2 and 3 x 3 matrices

2 x 2: simply multiplying the values in the diagonals and subtracting them.

|A| = |u1 u2| = u1*v2 - u2*v1   (imagine how you take out the values diagonally squish
      |v1 v2|                    them in your palm, finally you make them fight subtracting
                                 each others troops)

before the awesome 3 x 2, check out the single element matrix:

|A| = |u1| = u1   which makes sense, since the identity matrix would be 1, and you can't
                  perform any transformations(arithmetic operations) that could convince a 0
		  to be a one (remember det(A)=0 means, unsolvable as far as we're concerned)

finally, the cool one:

|A| = |u1 u1 u1| = u1(v2*w3-v3*w2) + u2*(v3*w1-v1*w3) + u3(v1*w2-v2*w1)
      |v1 v1 v1|
      |w1 w1 w1|



yeah.. never mind that, check this out this is an identity of the above:

|A| = |u1 u1 u1| = u · ( v x w)
      |v1 v1 v1|     ^     ^
      |w1 w1 w1|     |     cross-product
                     dot-product



v x w returns a vector perpendicular to the plane that v and w span. Now the dot-product
of this perpendicular vector with u determines the cos angle between the two. Remember how
a determinant of (0.0, 1.0] means the the matrix equations are solvable? Well guess what,
this is the case when u and the plane-normal (from v x w) are perpendicular!  And this is
exactly the case if u lies in the same plane as v and w! Visually this is simply the case
when all three vectors share the same plane - or - if we _lose a degree of freedom (make
the gimbal mind mapping here!) (dimension) because one of the vectors collapses into the
plane of one of other two!!

note also how the cross-product for two linearly dependent vectors (parallel to each
other: either face the same direction or the opposite) is also 0! Which makes sense no
plane is being span, we lose a degree of freedom! Now a dot-product with vec(0,0,0) - a
point - is always 0 (we failed to span the initial plane, no 3rd dimension for you!).

Now we can see the broad pattern here: Whenever we lose one of our total number of
available dimension/degree of freedom the determinant is equal to 0! You should be able to
see it clearly now before your eye:

1 x 1 matrix: a single vector is 1 dimensional, if it equal to 0, it no longer has extend in
any dimension geometrically and is just a point

2 x 2 matrix, we can quickly see if we have linearly dependent vectors or, if a row is 0,
a point and a vector and hence lacking the ability to span plane.

3 x 3 like described above

Finally, imagining the inverse matrix solution:
If we try to yield an identity matrix why is a determinant of 0 indicating that there won't
be a solution? What is the big deal with lacking a degree of freedom there? We can imagine
this using 1d-matrix that is just a vector whose determinant != 0 which is the case if
vector is "line". Now if were to transform an object using this 1d-matrix which is basically
a scalar, we could stretch the object or shrink it in this one dimension. And because we
have the magnitude with which we stretch the object - the scalar - we can transform it
back to an identity matrix:
scalar 3 ; /3
scalar 1 <- identity matrix

But what happens if the degree of freedom drops to zero? The vector (0) completely annihilates
the object (when being multiplied). Now how can we restore the "aspect" ratio of such an
object back to identity if we have lost the leverage, the dimension, to do so? We have no information
to make it back. This is why we can't inverse this such a matrix. (this might be related to
"covering map" and "homomorphism" and probably also with "gimbal lock").



cross product parallelogram,
the magnitude of the cross product is equal to the parallelogram with the vectors used for sides
but that's nothing:

signed area and determinant,
the determinant corresponds to the
- singed length (1d-matrix determinant)
- signed area of the parallelogram (2 x 2)
- signed volume of the parallelepiped of the (3 x 3)
- or the signed hyper-volume (!) of the n-dimensional hyper-parallelepiped

The parallelogram takes the two vector rows unites them at origin (0,0) and projects the parallelogram
from their respective tips. The parallelepiped is that parallelogram plus the columns from the
third vector on top of it. The hyper-parallelepiped would be a n-dimensional beast unfathomable for
us humans.

again note how we can't build any of these objects when a dimension is lacking: det(A)=0

--------------------------------------------------------------------------------
Determinant identities

|A| = |A^t|   ;; should be obvious now with the above signed area explanation

If B is obtained by interchanging the two rows or two columns of A, then |B| = -|A|
again obvious now, when we consider that v x w, w x v, an opposing vector
(use the _right-hand rule_!!)

This one is difficult to wrap my head around for now, makes sense for translating rotating
but not quite for scaling. Apparently multiplying two matrices and yielding the determinant
produces the same parallelepiped as just multiplying the determinants of both matrices:

|A|*|B| = |A*B|


Another novelty:
You can obtain B from A by multiplying _a single column/row_ by a scalar k. This can't be
done with matrix with matrix multiplication. This method of matrix permutation is new to me
and must be noted!
Given this, the following is trivial:
|B| = s*|A|    ;; s being the scalar
We can easily imagine just growing a single vector is growing the parallelepiped by the same
factor as just multiplying the whole thing by the factor that grew the individual vector.
They're both just factors in a formula comprised of only multiplication equation
So a cube of side 3*3*3 = 27, we grow a single vector 3*3*4 = 36, now growing the vector
required a factor of 4/3 because 4/3 * 3 = 4. And now the determinant multiplication:
(3*3*3)=27 * 4/3 = also 36

if A contains rows who are multiplies of columns or vice verse then |A|=0. Trivial, if they're
multiplies then they're linearly dependent -> freedom loss -> no dimension to span a plane or
grow a parallelepiped -> volume,area,segment = 0 = determinant

is any column/row zero then of course |A|=0, again dimensions drop -> ..

--------------------------------------------------------------------------------
Another way to calculate determinants:
_"use column and row operations"_(?) on the matrix to reduce it to a triangular matrix.
Now the product of the main diagonal entries is the determinant. BUT the regular u · w x v
STILL works!!

Now how can this be that we can willy-nilly add,subtract and multiply rows to each other
and still yield the same determinant? If we consider the cross-product and the
parallelogram that the resulting vectors magnitude represent we can test that these kind
of reduction operations hold true

(cross-product (vec3 3.0 0.0 0.0)
	       (vec3 0.0 3.0 0.0)) ==> #(0.0 0.0 9.0) ;; clearly the magnitude is 3*3=9

now adding one row to another is directly pertaining these vectors so:

(cross-product (vec+ (vec3 3.0 0.0 0.0)
		     (vec3 0.0 3.0 0.0)) ;; adding b to a here, just like adding for example  row 1 to 2
	       (vec3 0.0 3.0 0.0)) ==> #(0.0 0.0 9.0)  ;; it still holds true!!

;; now adding -0.5 the b to a+b
(cross-product (vec+
		(vec* (vec3 0.0 3.0 0.0) -0.5) ;; -0.5 b
		(vec3 3.0 3.0 0.0)) ;; already a+b
	       (vec3 0.0 3.0 0.0)) ==> #(0.0 0.0 9.0)  ;; still identity!!

But the thing to remember here is that while we can perform these identity reductions
we can't use those when dealing with matrices that are used for rendering and the transformations:

(1 0 0
 0 1 0  ;; identity matrix, now lets add the upper one to the lower one
 0 0 1)

(1 0 0
 1 1 0  det(*) => 1.0 "nothing happens" determinant wise, but as you can clearly see the
 0 0 1)               base vectors aren't all perpendicular and you get a distortion when you
                      use this matrix to render objects by multiplying a stream of vertices with it


/expansion by cofactor/,
the final procedure to calculate co-factors. We need a minor, which is a sub-matrix which we get
when we remove row i, and column j. Now we kind of cheat because we calculate the determinant
of the sub-matrix called /minor/ like described above (maybe the idea here is that it is much easier to calculate
the determinant from a lower dimension matrix). Then we detriment the algebraic sign depending on
the column/rows and pack it with the /minor/ to form the /cofactor/:
c.ij = (-1)^i+j*minor-matrix   ;; <- cofactor

Now all we need is to iterate through the matrix either column- or row-wise

     n             n
|A|= Σ a.rj*c.rj = Σ a.ik+c.ik
    j=1           i=1

a.rj just represents the element from matrix A at row column r,j or i,k
left: row-version, r corresponds to a row
right: column-version, k column

this is what it looks in action:

                        row=1 is cut out for all /minor/ -the determinant sub-matrix- and
                        we iterate through the columns i, this is the 2nd iteration so
			c.1,2=(-1)... and this -1 is taking its action as the sign for the whole
		a.rj    part here
    | 4 -2  6|  ↓       ↓
|A|=| 2	 5  0| =4|5  0|	-(-2)| 2  0|+6| 2 5|
    |-2  1 -4|	 |1 -4|	     |-2 -4|  |-2 1|
		 ^^^^^ at these we cheat and either just perform the det(*) tests described above
                       or we recursively apply the same trick (remember there can be n x n matrices)
		       this is because, again, /minor/ _are_ determinants of sub-matrices defined as
		       the matrix yielded by removing one row and column from A.
	       =  -80   -    16     +  72  = -24 ;;<- and indeed the determinant is -24

--------------------------------------------------------------------------------
Determinant applications!

Cramer's rule - introduction work:

solving a system of linear equations

ax+by=e  
cx+dy=f  can be expressed as:

A = [a b  , X = [x   , B = [e
     c d]        y]         f]


where A is the /coefficient matrix/
X is the /solution vector/
B is the /constant vector/

above linear equation system can be now written as:
  AX=B ;; x A⁻¹ <- inverse
and solving for X, ideally the single point where two lines intersect:
  X=A⁻¹*x
From this we can see that to get the solution we must know of A is actually _invertible_!

That's all well and good, but let's first consider how do we actually represent lines
using the above two linear equations?
Most people will be familiar with the slope-intercept notation:

y=xm+b  ;; where m is the slope and b the intercept (the point where the line crosses the y-axis)
        ;; note: the slope-intercept form _can't represent vertical lines!_ 

Real example m=0 and b=1
y=1 which is a line parallel parallel to the coordinate x axis and offset by one, and it can
be easily formulated into a function f(x)=1
But how do we use this representation with a form like:

  ax+by=e

First of what's up with the a, b and e? Well that's what you get when you algebraically manipulate
our fine slope-intercept form:

ax+by=c ;-ax
by = -ax + c  ;; and there is out "y=mx+b"

Now lets try to transform these slope-intercept form into general linear equations

(1) y=2     ->   0*x+1*y=2
(2) y=x ;;-x -> -1*x+1*y=0
which have a point in common at P(2,2), so let's see if we can find a solution using the matrix
form

 0*x+1*y=2 into matrix-form->  [ 0 1  x  [x    = [2
-1*x+1*y=0                      -1 1]     y]      0]
                                 A    *   X    =  B
So lets see if A is invertible, so we can solve for X:
|A| = | 0 1| = -1 invertible!!
      |-1 1|

well lets cheat here and get inverse matrix without a formal way:
A⁻¹= [1.0 -1.0
      1.0 0.0]

Now we plug this into the equation X =A⁻¹B
[1.0 -1.0 x [2  = [2
 1.0 0.0]    0]    2]   and indeed X = [2
                                        2]
Which is the intersecting point of y=2, y=x!!!

--------------------------------------------------------------------------------
An intuitive explanation of ax+by=c
the general form of a line above, the linear equation of two unknowns x and y
has, thanks goodness, a visual and intuitive explanation!

Consider the line
y=x+2  and its linear equation representation:
-2+y=2

What we first want to do is get the matrix row out of there
   a b c into
 [-2 1 2]

the first two values a and b form a 2d-vector [-2 1] which is _perpendicular to the line we're
defining here_(!), while c tells us _where the perpendicular line intersect with the
line we're defining here_(!).
Now you should see the perpendicular line and how it raises to y=2 and the line
perpendicular to it is indeed y=x+2!!


delving into the third dimension..

the form can be extended to three unknown, but this is _not a 3d line_ it is a plane(!)

  Ax+By+Cz = d

here the vector [A,B,C] is perpendicular to the plane, like the normal of a surface. To
get a line, we need to intersect two planes, for which we need two equations:

aka two planes make a line. A plane can be represented by 3 non-collinear points, but we
the method above, where we take a vector (x,y,z) and the normal to of the plane (A,B,C)
and this also is enough to represent a plane. The vector (x,y,z) is on the plane if the
dot-product, which is implied being calculated in Ax+By+Cz, between (A,B,C) and it
equals 0 (d = 0). So now if we plug in values for x,y,z we can test if they're on the plane
if the equation equals 0. Now to get a line we want to plug in the same values into another
plane definition of the form Ax+By+Cz=d if they're also equal d=0 for the same values
then we'd have found a point in common on both planes. Good, good. Now to get a line of
points in common is also the definition of a plane.

(1) A1x+B1y+C1z=d1
(2) A2x+B2y+C2z=d2    How do we solve this _linear equation system_? Well forget that for
                      now. We have the normals for both planes readily available all
we need to do is just calculate the cross-product of the two [A1..]x[A2..] and we get
a vector that is collinear with our line!! Now we just need to put this line somewhere,
a point to center it on. This can be calculated by solving for a point they have in common.
A solution involves just taking in one of the summands and set its coefficient x,y or z
to 0, and solve the equation such that we get a point to center our cross-product line
on it.

See below why a point and the collinear line suffices for the representation.

--------------------------------------------------------------------------------
Geometric representation of lines with vectors
   
v1=[x,y,z] v2[x2,y2,z2]

the set of lines, L, would be represented as: L={v1+s*(v1-v2)| s ∈ R}
If we write this in _parametric form_:
  x = v1.x e+t*(v1.x-v2.x)
and analogous for the other coordinates

--------------------------------------------------------------------------------
Now let's see what this would look like for a linear equation with infinite solutions
(parallel + coinciding lines)

(1)y=2, (2)y=2 both's general linear equation is: 0*x+1*y=2

|0 1 = determinant is 0!
 0 1|  no plane is span!

So we can't even calculate the inverse for the X=A⁻¹*B. Let's solve these equations the
old fashioned way:

(1) 0*x+1*y=2 => y=2
(2)     "        y=2
(1)-(2): 0=0 and indeed, "infinite solutions"

The book derives the AX=B form by stating that
x = (de-bf)/(ad-bc)
where a,b,c,d are the elements of the A matrix and e="c" of (1) and f="c" of (2)
so we can substitute
x = (1*2-1*2)/(0*1-1*0)
x =  2-2     / 0  ;;<- boom division by zero!! In this case, geometrically,
                  ;;   the lines have infinite common points, or every point of one
		  ;;   line is common with the other line
                 
--------------------------------------------------------------------------------
Now the two parallel, non coinciding lines
(1) y=2 (2) y=3

(1) 0*x+1*y=2
(2) 0*x+1*y=3
and we can already see nothing changed on the left side in this particular example
but the determinant is 0.0 and again the above x solution is going to be a division
by 0.

conclusion: If the determinant, of the coefficient matrix, is not zero, then the linear
equation system has one unique point in common. This also means that the collision test of
_whether_ a collision has occurred is already solved when det(matrix) != 0 is true!

--------------------------------------------------------------------------------
Cramer's rule,
solves linear equation system of arbitrary size (for example also 3 x 3). It tells us if a
collision between objects represented by those equations of the same arbitrary dimensions
has occurred and tells us where.

The If: when the coefficient matrix determinant is non-zero

The where:

given an linear equation system,
a1x+b1x+c1x=d1
a2x+b2x+c2x=d2
a3x+b3x+c3x=d3



represented as a matrix, and in determinant form:

|a1 a2 a3| 
|b1 b2 b3|   = our coefficient matrix d,
|c1 c2 c3|

In the above examples we solved this with X=A⁻¹*B where B would be the vector
[d1,d2,d3]. We need to keep this in mind as for performance reasons. But Cramer's rule
allows us to get the individual elements of the unique point P(x,y,z), the point that
solves all equations, by providing equations that solve for each of the points.

Getting the points is just replacing the column in order of the component one wishes to
compute by the constant vector, compute the determinant from that and have it be divided
by the coefficient matrix' determinant.

Performance notes: for systems with more than 3, 4 equations /Cramer's rule/'s
amount of work increases drastically. The author recommends using the
/Gaussian elimination algorithm/(!!) But for smaller system of linear equations
the rule is recommended as it is easy to apply, also the ability to calculate
individual components of the unique point is stressed.
Just to compute the solution of 4 linear equations 360 multiplications, 4 divisions
and 115 additions are necessary.

--------------------------------------------------------------------------------
cross-product:

[4      [12     \       ;; where i is repeating {1,2,3}      /
 3   x    6      \                                          /
 2]       9]      *---> v1.i * v2.i+1  - v2.i * v1.i+1 <---*
                      = v1⨯v2.i+2

(- 27 12)  15
(- 24 36) -12
(- 24 36) -12

--------------------------------------------------------------------------------
Inverse, defined as:

A⁻¹= 1/det(A) ;; <- again the determinant being the denominator cannot be zero

--------------------------------------------------------------------------------
Determinant predicate,
like in cl predicate implies that we will test for something, as seen with the sphere
formula or if something is solvable like with inverting or solutions to linear
equation systems with Cramer's rule.
_Most geometrical tests can be cast in determinant form_(!) Because of this determinants
have been heavily studied including performance computing solution.

sign of the determinant (+,-),
plays an important role and can imply:
- orientation (ORIENT2D function below)
- sidedness (also ORIENT2D)
- inclusion of points (sphere)

performance correctness trickery:
- look out floating point error amounting from arithmetic operations that are part of the
  determinant calculation
- going straight for the determinant is performance-wise usually a bad idea, better:
  divide-and-conquer or subexpression elimination like in Cramer's rule

--------------------------------------------------------------------------------
orient2d(A,B,C), 
takes 2d-points A,B,C ; is an example of a predicate determinant for sidedness AND
orientation of points

calculates a determinant predicate:

                  |ax ay 1|
orient2d(A,B,C) = |bx by 1| = |ax - cx   ay - cy|
                  |cx cy 1|   |bx - cx   by - cy|

Which has _many_ properties:
		  
orient2d(A,B,C) > 0, C lies to the left of the directed line AB. And as a plus this
also means the triangle is oriented counterclockwise (ccw).

orient2d(A,B,C) < 0, C right of the line; triangle CW.

orient2d(A,B,C) = 0, points are all collinear!

the actual value returned corresponds to twice the signed area of the triangle (negative
if triangle is CW)!

--------------------------------------------------------------------------------
orient3d(A,B,C,D),
here we deal with supporting plane of the triangle ABC.
"above the plane" means that from D view the triangle is counterclockwise
< 0 :  D is above the ABC plane
> 0 : below
= 0 : coplanar

It is the calculation of the 4d-matrix determinant where the first three columns are
Point's x,y,z values and the 4th being 1.

|a.x a.y a.z 1|
..
|d.x d.y d.z 1|

The value returned is _six_ times the signed are of the tetrahedron formed by A,B,C,D

--------------------------------------------------------------------------------
incircle2D(A,B,C,D),
the points of the CCW triangle ABC are on a circle, then
> 0 means that D lies inside the circle 
< 0 D lies outside
= 0 the four points are cocircular!
If the triangle is CW (same as orient2d(A,B,C) < 0) then the results are reversed

computation:

incircle2d(a,b,c,d) = |a.x ay a².x+ a².y 1|
                      |b.x by b².x+ b².y 1|
                      |c.x cy c².x+ c².y 1|
                      |d.x dy d².x+ d².y 1|

--------------------------------------------------------------------------------
insphere(A,B,C,D,E),
the tetrahedron's ABCD points lie and define the surface of the sphere, like
with incircle depending on orient3d(..) the opposite holds:

> 0 E lies inside the sphere
< 0 E outside
= 0 all points are /cospherical/

For INSPHERE and INCIRCLE2D the actual value has no special value.

--------------------------------------------------------------------------------
(2d) /pseudo cross product/, also: /pseudo dot-product/,
u⟂⋅v, where u⟂ (read: "u-perp" is the CCW perpendicular vector to u.
corresponds to: the signed area of the parallelogram formed by u⋅v. Positive
if v is CCW from u. Zero if they're collinear.

--------------------------------------------------------------------------------
Scalar Triple Product, or /triple scalar product/, or /box product/ [u v w],
because the operation:
  (u ⨯ v) ⋅ w
occurs a lot, it has its own name. Note that this is our determinant computation
of a 3x3 matrix from before(!) yielding the singed area of a parallelepiped,
also six times the size spanned u,v,w tetrahedron, using an additional point
to unite them at origin we recognize our Orient3d() function.

It also written as [u v w] and it holds true for any cyclic permutation

  (u⨯v)⋅w = (w⨯u)⋅v = (v⨯w)⋅u  !

note permute the vectors and then cycle the negations the following holds true

 -[uwv] =  -[vuw] =  -[wvu]

--------------------------------------------------------------------------------
/barycentric coordinates/,
we use reference points to represent points in the space enclosed by them - "parameterize
the space". Two points form a line, three a triangle. The representation allows then to
get at the points on the line or on the surface of the triangle.

A triangle ABC:
  A + u(B-A)*v(B-C)   ;;<- (u,v) are the barycentric coordinates, where if
                      ;; 0 <= u,v <= 1.0 the point is within the triangle

Imagine A to be the origin of a coordinate system of base vectors AB and BC.  Remember the
notes in arcsynthesis on the sin/cos brothers who when they work together give and take in
equal amounts: Imagine as "u" shrinks the AB vectors gets more directional and magnitude
influence from the BC vector and vice versa. See how it all grows out from the tip of the
A vector!
It should be easy now to see if a point is within the triangle, that is, whenever
0 <= u,v <= 1.0 is satisfied.

Uses of barycentric coordinates:
Given the BARYCENTRIC implementation we can test if a point P is inside a triangle ABC.
Also the use by OpenGL should be intuitive by now. All the things encountered in the
Arcsynthesis-study, or simply put whenever we needed the interpolated values from a
triangle like colors (Gouraud shading), normals (Phong shading) or texture texels/pixels
(texture mapping)!

--------------------------------------------------------------------------------
Lines, Rays, and Segments

consider the definition of a /line/ of two _distinct_ points

L(t) = (1-t)*A+t*B

the peculiar (1-t) indicates that if t=1 the point on the line is B:
= (1-1)*A+1*B
=   0  *A+B
= B

conversely if t = 0 the point on the line is A
= (1-0)*A+0*B
=   1  *A
= A

it should be easy to see how t grows, shrinks and inverts the A+B vector in effect
pointing at any point on the line.


/line segment/, or just /segment/,
is the portion connecting A and B which when we look at the t=0 and t=1 value computed
above is simply defined as the range
  0 <= t <= 1 !

  
/directed segment/,
if the endpoints A and B of the segment are given with a definite order in mind


/ray/,
is a /half-infinite/ line!! This is quite a helpful visual explanation because t is
only limited by
  t >= 0
i.e. only one direction is limited, namely below 0, while the other is infinite -
half-infinite!!


The parametric equation of the line L(t) above can be rearranged into:
= (1-t)*A+t*B
=  A-tA+t*B
=  A+tB-tA  //*swap*
=  A+t(B-A)

  hence we get:
                         L(t) = A + t*v    (where v = B - A)
                             ^^^^      
_Rays are usually defined in this form above_!!


parameteric equation of the line,
the two representation of a line shown above:
  L(t) = (1-t)*A+t*B
and
  L(t) = A + t*v

--------------------------------------------------------------------------------
Planes,
- represented by three non-collinear points: 

P(u,v) = A + u(B-A)+v(C-A)            (parametric representation)

remember the non-intuitive order of vector substraction "B-A" indicates a vector
from _A_ to _B_, then, the above equation represents a flow from A into B and C
spaning a plane, and triangle with parametrized B-A C-A sides by "u" and "v".


/in front the plane/,
given a triangle ABC the convention is that when viewed /in front/ 
- the _points are CCW_ and
- the _normal points towards the viewer

conversely /behind/ the plane is the opposite
In OpenGL we use this property to cull triangles from the rendering process. This
is efficient as only one side of a triangle can be viewed and we can form meshes
with clock-wise similar triangle arrangement on their surface so the rendering
engine can safely cull the invisible insides from the computation.


