Notes on: "real-time collision detection" by Christer Ericson


-- MATH PRIMMER --



explicit representation,
objects represented by polygons, through their points, edges and faces

implicit representation,
geometrical objects such as spheres, cones, cylinders, tori (pl. torus), etc. which
can't be explicitly represented but implicitly through a mathematical expression:

implicit representation are usually represented as a mapping from 3d-space to 1d-real-numbers
like so:

[first, new, cool, math thing!]

f:R^3 -> R, where points given:
f(x,y,z) // our R^3
will evaluate into three regions zero, positive and negative:

f(x,y,z) = 0 // boundary of the object
f(x,y,z) > 0 // exterior of the object
f(x,y,z) < 0 // interior of the object

awesome!

Example for a sphere:
x^2 + y^2 + z^2 <= r^2    // works!

This is like a double Pythagorean theorem on the left side the resulting
distance is, if equal r^2, a point _on_ the sphere

(defun sphere-collision-p (x y z sphere-radius)
  "Returns true if the point P(x,y,z) lies within or touches the sphere of radius:
   sphere-radius."
  (<= (apply #'+
	     (mapcar #'(lambda (x) (expt x 2))
		     (list x y z)))
      (expt sphere-radius 2)))

Through understanding that this is just a double Pythagoras we can formulate a collision
test for a 2d-circle easily:
x^2 + y^2 <= r^2  (awesome number two!)

An object boundary defined in such a way is called an /implicit surface/!

Advantages:
- quick rejection culling (is it in the interior = CULL it out!)
- fast intersection test: well simply feed a point into the f(x,y,z) function and you know
  if it is inside or touching the boundary: f(x,y,z) =< 0!

half-space,
we cut space in two along an infinite plane. The resulting two spaces left and right of
the plane are infinitely large but still just half-spaces of a bigger infinity!

convex objects (polygon),
any two points within this object can connect in a straight line without leaving the region of
the object. We need this word because collision tests are much easier with such objects!
Foreshadowing: we will probably try to break concave polygons into smaller convex objects?

concave polygon object,
opposite of convex object. Contains an interior angle greater than 180-degrees.

half-space intersection representation (convex objects only),
another way to represent polygonal objects, here we cut away some space with planes and use
the resulting half-space intersections to represent an object. An example would be to represent
a rectangle in 2d by using two x-axis values to cut away the two sides - resulting in an
infinitely long rectangle along the y axis- then provide two y-axis to encompass a finitely
big rectangle! (see texatl.cl:space for this kind of rectangle representation!)
This is reminiscent of the aabb test!

--------------------------------------------------------------------------------
We won't usually use the same geometry for collision as for rendering
Pros:
- it is usually too complex, and collision test don't need to be that precise
- the usual triangle representation of objects are harder to query for collision
  then other shapes like spheres, for example
- rendering data may be organized in quite a different way than is useful for
  collision. Collision tests probably want spatial hierarchical relationships
  whereas rendering data will create topologies for adjacent vertices at best
  and material attributes, color and texture coordinates at worst (useless for
  collision tests!)
- organizing for collision tests makes the data smaller hence more efficient to
  use and coherent in memory and useful for further extension and tests tailored
  for collision
- sometimes you want to explicitly differ, for example the plane representing
  snow might be intentionally lowered in the proxy geometry to allow knee-deep
  movement in the snow. Instead for exclusion tests, just use a different geometry
  to begin with.
- even if rendering data is not needed because an object is not visible it is
  still needed for collision. Just imagine walking behind a wall, where the
  camera sees the wall it will usually cull out everything behind it, you should
  however still follow the law's of physics even when nobody is around to see it!

Cons:
- Data duplication. Could be solved by inferring the proxy geometry from the rendering
  geometry (through linearization caching)!!
- time to build the tools that generate the geometry or more time for the designer
  to create his models
- visual consistency may suffer: objects may float above others, or fully intersect
  (surely you seen the animations which simply move through anything when parts
   of it move outside the hitbox)
- interdependency problems: if the rendering geometry changes, the collision geometry
  might also have to change, and how is this process automated. Which of the two
  geometry comes first?

we use, hence, another geometry instead.. as a proxy:

proxy geometry,
the collision test geometry for objects. This usually reduces the complex rendering
geometry by encapsulating a object of interest into a:

bounding volume,
a simpler geometrical object like a box or a sphere representing the object in a
simpler way than the actual rendering geometry for the sake of a simpler easier
collision test!

--------------------------------------------------------------------------------
Types of queries:

interference detection or intersection testing,
yes/no did the objects collide, do they intersect?
Easy to implement, commonly used

intersection finding - WHERE did they intersect?
One contact point may sometimes suffice other times, as in rigid body simulations,
we need to detect a whole set of contact points /the contact manifold/!
Calculating the contact manifold may be difficult, and approximations may be used -
a common practice in games.

collision properties,
special collision queries, taking surface properties into account like slipperiness of a
road and climbability of a wall

penetration depth,
how deep are two objects overlapping. The problem here is what start and end point to
use to measure this. A solution to this can be the shortest movement vector needed
to separate the already interpenetrated objects. Its computation is also a difficult
problem.

/separation distance/, has a beautiful definition:
"The separation distance between two disjoint objects A and B is defined as _the minimum
of the distances between points in A and points in B_"!
Needed the predict the time (WHEN) two objects will collide. Also it poses another
problem of finding the

closest point,
in both the disjoint object A and B to each other. There can be infinite closest points!

ETA, /estimated time of arrival/, or, more dramatic, TOI, /time of impact/

--------------------------------------------------------------------------------
pairwise collision tests for n objects, take O(n^2) time - every objects with
every other object n x n!
But we can use a divide and conquer approach:

broad phase or n-<body> processing,
use a rule to decide which objects /may/ collide. For example divide a scene into
square regions, and whenever an object is inside a given region it is mapped to
some square-region data structure. This data structure can then exclude all the
objects that are sole inhabitants of a region from any collision test. The
other objects under go the

narrow phase (pair processing),
perform all the pairwise test on the unconquered parts (those objects
sharing the same square region!).

--------------------------------------------------------------------------------
Sequential vs simultaneous motion,
for both we break down movement to the unit of _time step_(!). Simultaneous motion
is more realistic but more expensive to compute:
(1) determine earliest time of contact
(2) move all objects for the extend of the no-contact-time
(3) resolve the collision
(4) repeat for the extend of the time step
A problem occurs when the object moves along a surface, then a collision resolution
would take place all the time and the simulation would only advance by a small
fraction.

While broad phase grouping can help divide and conquer the problem.  An alternative
simultaneous motion solution: advance the objects by a fixed small time step, when an
interpenetration of objects during a time step occurs restore the state before the
time step. This is also expensive.


The accuracy of simultaneous motion should be reserved for rigid-body simulations.
For most program sequential motion suffices (including games).

Sequential motion accuracy problems:
(1) Two objects during motion, in a sequential approach one object might move past the
collision point, then the other object also moves past it -> no collision occurs. (2) One
object is just behind another object, when it moves first it will catch up but if both
move at the same speed this catching up wouldn't make sense.  Both these issues _can be
solved by just making the time step smaller_!
In games the small time step is easily provided by the _high frame rate_(!)

sequential motion benefits include easy penetration resolution: just undo the single
motion during the small time step, this is to be contrasted by the simultaneous where we
have to undo motion during a time step for _all_ the objects (possibly minus some
broad phase excluded ones)!

--------------------------------------------------------------------------------
Discrete vs continuous motion

Discrete motion - static collision detection,
Collision detection is measured at a specific time, and the objects are treated as if
they're frozen in said time (This is much cheaper than dynamic collision detection below)
The object effectively teleports from time step to time step, the collision test is hence
less precise. On the note of teleportation: the time step teleportation distance has to be
smaller than the objects spatial extend or else the object might teleport through other
objects with no collision being detected, this is aptly called /tunneling/!!!

Excursion on tunneling:
To have more fun memorizing this, in quantum mechanics teleportation is an everyday thing
performed by particles all the time, called quantum tunneling. According to QM electrons
perform quantum leaps to move between energy states in atom, as the wave(wave-particle
duality) travels around the nuclei of the atom it reinforces itself on each iteration
thereby maintaining its energy level.

Another exciting thing: scientist were able to quantum tunnel a particle over the distance
of 9 meters. But this doesn't mean teleportation of humans is gonna any time happen soon,
when prompted for this an expert said that they had to get the particle in a particular
state before performing this, and the particles in human bodies just can't be manipulated
into such a state anytime soon.
/Excursion

dynamic collision detection,
we detect the collision by considering the continuous motion - meaning over a given time
interval or between start and end of a time step. This is both more accurate and more
expensive. More accurate because we can usually tell the exact time and point(s) of first
contact.

swept volume,
in continuous motion, this is what we refer to the space carved out by an object as it
moves for the extend of a single time step! This can be used for the broad phase: only if
the two objects' swept volumes do intersect do they become member of the same narrow phase
group. Of course even then collision is not guaranteed (should be easy to imagine
why). But imagine an object moving in a non-trivial path during the time step, the swept
volume can then be hard to compute and work with! Fortunately such accuracy is rarely
needed, and can be reduced for simplification to a linear motion.

speedbox,
in games the swept volume is usually simplified to an elongated box covering the objects
full range of motion (or some similarly simple proxy object).


relative motion,
what-is (high-level explanation):
subtracting two motion vectors hence one being stationary and the other moving towards it
for simpler intersection finding of the swept volumes. Consider v1 v2, for v1-v2, v2 is
the stationary "observer" where v1-v2 is the vector that moves towards it from v1 while,
again, v2 can't move. This explanation though is enough to work abstractly I want to
understand this at a visceral - intuitive level.
how-to (how to implement it explanation): (TODO: test this)
given two motion vectors v1 and v2 it is virtually always better to consider the relative
motion:
vp1 vp2 are position vectors, and v1 v2 are these vectors' motion (direction) vectors
the relative motion is the subtraction: v1-v2
The observer is the right-hand vector: vp2
it is immobile as vp1+(v1-v2) moves towards it (as far as the time step allows).
This is better because we now only have the swept volume carved out by vp1+(v1-v2)

Optimization,
is mainly achieved by the broad phase, putting objects into regions or region hierarchies that
may even be recursively extended when more objects govern a certain region (implementation
example: quadtree).

Robustness,
two problems: floating point imprecision and geometry nonsense through transformation. One
can give rise to the other, for example: space inverting on itself, through interpolation
off-by-one bug turning into extrapolation; or transformation order is violated and rotation
scaling distorts the geometry, or transforming normals on a non-uniform object (transpose
(inverse norm-matrix)), or systems prone to gimbal lock may have unexpected orientation at
certain stages of interpolation.  The collision system must be prepare and avoid those for..
robustness!


------------------------------------------------------------------------------
Math and geometry primer


transpose matrix of A: A^t

/skew symmetric/ or /antisemitic/ matrix,
when A^t = -A. For example rotation about the x-axis, then the antisemitic matrix
would be a 180-degree turn around the x-axis

;; 90-degrees              ;; 180-degree of 90-degrees turn before
#<[1.0, 0.0, 0.0, 0.0]     #<[1.0, 0.0, 0.0, 0.0]
  [0.0, 0.0, -1.0, 0.0]	     [0.0, 0.0, 1.0, 0.0]
  [0.0, 1.0, 0.0, 0.0]	     [0.0, -1.0, 0.0, 0.0]
  [0.0, 0.0, 0.0, 1.0]>	     [0.0, 0.0, 0.0, 1.0]>

matrix addition,
pairwise addition of corresponding positions. Given matrix A=[a.ij] and B=[b.ij]
then the sum matrix C=[c.ij] = A+B is defined as the pairwise addition of elements:
c.ij = a.ij + b.ij


lower triangular matrix,
all the entries of the matrix above the main diagonal are zero

upper triangular matrix,
opposite of lower triangular matrix

row/column vector or row/column matrix,
an m x n (m by n) matrix is comprised of smaller row/column matrices that make
up the eponymous rows/columns of the matrix. Also called the row/column vector.

singular (or noninvertible),
matrices that do _not_ have an inverse. For example: any matrix that has an all-zero
row.
Or more formally: if the determinant of matrix is 0, then the matrix is noninvertible.


Algebraic identities involving matrices,
given scalar r and s and matrices A, B and C (of appropriate sizes required to perform
the operations) the following identities hold:

s(A+B) = s*A + s*B     ;; i.e. scalars can be factored out
A(s+r) = r*A + s*A     ;; matrices too

scalar*matrix multiplications are associative
r(s*A) = s(r*A) = (r*s)A

A(B+C) = AB + AC

(s*A)*B = s(A*B) = A(s*B)  ;; tricky

transposing,
(A+B)^t = A^t + B^t  ;; is distributive:
(s*A)^t = s*A^t      ;; doesn't mind scalars
(A*B)^t = B^t*A^t    ;; switches the order when dissolving parens! tricky!!


--------------------------------------------------------------------------------
Determinants det(A) or |A|
it is a _single number_ associated with the matrix and it tells us if the systems of
linear equations, that matrix is comprised of, is _solvable_. It is solvable when the
values is det(A) != 0. One of the things we want to solve is if the matrix is
_invertible_.

We will mainly need to calculate the determinant of 2 x 2 and 3 x 3 matrices

2 x 2: simply multiplying the values in the diagonals and subtracting them.

|A| = |u1 u2| = u1*v2 - u2*v1   (imagine how you take out the values diagonally squish
      |v1 v2|                    them in your palm, finally you make them fight subtracting
                                 each others troops)

before the awesome 3 x 2, check out the single element matrix:

|A| = |u1| = u1   which makes sense, since the identity matrix would be 1, and you can't
                  perform any transformations(arithmetic operations) that could convince a 0
		  to be a one (remember det(A)=0 means, unsolvable as far as we're concerned)

finally, the cool one:

|A| = |u1 u1 u1| = u1(v2*w3-v3*w2) + u2*(v3*w1-v1*w3) + u3(v1*w2-v2*w1)
      |v1 v1 v1|
      |w1 w1 w1|



yeah.. never mind that, check this out this is an identity of the above:

|A| = |u1 u1 u1| = u · ( v x w)
      |v1 v1 v1|     ^     ^
      |w1 w1 w1|     |     cross-product
                     dot-product



v x w returns a vector perpendicular to the plane that v and w span. Now the dot-product
of this perpendicular vector with u determines the cos angle between the two. Remember how
a determinant of (0.0, 1.0] means the the matrix equations are solvable? Well guess what,
this is the case when u and the plane-normal (from v x w) are perpendicular!  And this is
exactly the case if u lies in the same plane as v and w! Visually this is simply the case
when all three vectors share the same plane - or - if we _lose a degree of freedom (make
the gimbal mind mapping here!) (dimension) because one of the vectors collapses into the
plane of one of other two!!

note also how the cross-product for two linearly dependent vectors (parallel to each
other: either face the same direction or the opposite) is also 0! Which makes sense no
plane is being span, we lose a degree of freedom! Now a dot-product with vec(0,0,0) - a
point - is always 0 (we failed to span the initial plane, no 3rd dimension for you!).

Now we can see the broad pattern here: Whenever we lose one of our total number of
available dimension/degree of freedom the determinant is equal to 0! You should be able to
see it clearly now before your eye:

1 x 1 matrix: a single vector is 1 dimensional, if it equal to 0, it no longer has extend in
any dimension geometrically and is just a point

2 x 2 matrix, we can quickly see if we have linearly dependent vectors or, if a row is 0,
a point and a vector and hence lacking the ability to span plane.

3 x 3 like described above

Finally, imagining the inverse matrix solution:
If we try to yield an identity matrix why is a determinant of 0 indicating that there won't
be a solution? What is the big deal with lacking a degree of freedom there? We can imagine
this using 1d-matrix that is just a vector whose determinant != 0 which is the case if
vector is "line". Now if were to transform an object using this 1d-matrix which is basically
a scalar, we could stretch the object or shrink it in this one dimension. And because we
have the magnitude with which we stretch the object - the scalar - we can transform it
back to an identity matrix:
scalar 3 ; /3
scalar 1 <- identity matrix

But what happens if the degree of freedom drops to zero? The vector (0) completely annihilates
the object (when being multiplied). Now how can we restore the "aspect" ratio of such an
object back to identity if we have lost the leverage, the dimension, to do so? We have no information
to make it back. This is why we can't inverse this such a matrix. (this might be related to
"covering map" and "homomorphism" and probably also with "gimbal lock").



cross product parallelogram,
the magnitude of the cross product is equal to the parallelogram with the vectors used for sides
but that's nothing:

signed area and determinant,
the determinant corresponds to the
- singed length (1d-matrix determinant)
- signed area of the parallelogram (2 x 2)
- signed volume of the parallelepiped of the (3 x 3)
- or the signed hyper-volume (!) of the n-dimensional hyper-parallelepiped

The parallelogram takes the two vector rows unites them at origin (0,0) and projects the parallelogram
from their respective tips. The parallelepiped is that parallelogram plus the columns from the
third vector on top of it. The hyper-parallelepiped would be a n-dimensional beast unfathomable for
us humans.

again note how we can't build any of these objects when a dimension is lacking: det(A)=0

--------------------------------------------------------------------------------
Determinant identities

|A| = |A^t|   ;; should be obvious now with the above signed area explanation

If B is obtained by interchanging the two rows or two columns of A, then |B| = -|A|
again obvious now, when we consider that v x w, w x v, an opposing vector
(use the _right-hand rule_!!)

This one is difficult to wrap my head around for now, makes sense for translating rotating
but not quite for scaling. Apparently multiplying two matrices and yielding the determinant
produces the same parallelepiped as just multiplying the determinants of both matrices:

|A|*|B| = |A*B|


Another novelty:
You can obtain B from A by multiplying _a single column/row_ by a scalar k. This can't be
done with matrix with matrix multiplication. This method of matrix permutation is new to me
and must be noted!
Given this, the following is trivial:
|B| = s*|A|    ;; s being the scalar
We can easily imagine just growing a single vector is growing the parallelepiped by the same
factor as just multiplying the whole thing by the factor that grew the individual vector.
They're both just factors in a formula comprised of only multiplication equation
So a cube of side 3*3*3 = 27, we grow a single vector 3*3*4 = 36, now growing the vector
required a factor of 4/3 because 4/3 * 3 = 4. And now the determinant multiplication:
(3*3*3)=27 * 4/3 = also 36

if A contains rows who are multiplies of columns or vice verse then |A|=0. Trivial, if they're
multiplies then they're linearly dependent -> freedom loss -> no dimension to span a plane or
grow a parallelepiped -> volume,area,segment = 0 = determinant

is any column/row zero then of course |A|=0, again dimensions drop -> ..

--------------------------------------------------------------------------------
Another way to calculate determinants:
_"use column and row operations"_(?) on the matrix to reduce it to a triangular matrix.
Now the product of the main diagonal entries is the determinant. BUT the regular u · w x v
STILL works!!

Now how can this be that we can willy-nilly add,subtract and multiply rows to each other
and still yield the same determinant? If we consider the cross-product and the
parallelogram that the resulting vectors magnitude represent we can test that these kind
of reduction operations hold true

(cross-product (vec3 3.0 0.0 0.0)
	       (vec3 0.0 3.0 0.0)) ==> #(0.0 0.0 9.0) ;; clearly the magnitude is 3*3=9

now adding one row to another is directly pertaining these vectors so:

(cross-product (vec+ (vec3 3.0 0.0 0.0)
		     (vec3 0.0 3.0 0.0)) ;; adding b to a here, just like adding for example  row 1 to 2
	       (vec3 0.0 3.0 0.0)) ==> #(0.0 0.0 9.0)  ;; it still holds true!!

;; now adding -0.5 the b to a+b
(cross-product (vec+
		(vec* (vec3 0.0 3.0 0.0) -0.5) ;; -0.5 b
		(vec3 3.0 3.0 0.0)) ;; already a+b
	       (vec3 0.0 3.0 0.0)) ==> #(0.0 0.0 9.0)  ;; still identity!!

But the thing to remember here is that while we can perform these identity reductions
we can't use those when dealing with matrices that are used for rendering and the transformations:

(1 0 0
 0 1 0  ;; identity matrix, now lets add the upper one to the lower one
 0 0 1)

(1 0 0
 1 1 0  det(*) => 1.0 "nothing happens" determinant wise, but as you can clearly see the
 0 0 1)               base vectors aren't all perpendicular and you get a distortion when you
                      use this matrix to render objects by multiplying a stream of vertices with it


/expansion by cofactor/,
the final procedure to calculate co-factors. We need a minor, which is a sub-matrix which we get
when we remove row i, and column j. Now we kind of cheat because we calculate the determinant
of the sub-matrix called /minor/ like described above (maybe the idea here is that it is much easier to calculate
the determinant from a lower dimension matrix). Then we detriment the algebraic sign depending on
the column/rows and pack it with the /minor/ to form the /cofactor/:
c.ij = (-1)^i+j*minor-matrix   ;; <- cofactor

Now all we need is to iterate through the matrix either column- or row-wise

     n             n
|A|= Σ a.rj*c.rj = Σ a.ik+c.ik
    j=1           i=1

a.rj just represents the element from matrix A at row column r,j or i,k
left: row-version, r corresponds to a row
right: column-version, k column

this is what it looks in action:

                        row=1 is cut out for all /minor/ -the determinant sub-matrix- and
                        we iterate through the columns i, this is the 2nd iteration so
			c.1,2=(-1)... and this -1 is taking its action as the sign for the whole
		a.rj    part here
    | 4 -2  6|  ↓       ↓
|A|=| 2	 5  0| =4|5  0|	-(-2)| 2  0|+6| 2 5|
    |-2  1 -4|	 |1 -4|	     |-2 -4|  |-2 1|
		 ^^^^^ at these we cheat and either just perform the det(*) tests described above
                       or we recursively apply the same trick (remember there can be n x n matrices)
		       this is because, again, /minor/ _are_ determinants of sub-matrices defined as
		       the matrix yielded by removing one row and column from A.
	       =  -80   -    16     +  72  = -24 ;;<- and indeed the determinant is -24

--------------------------------------------------------------------------------
Determinant applications!

Cramer's rule - introduction work:

solving a system of linear equations

ax+by=e  
cx+dy=f  can be expressed as:

A = [a b  , X = [x   , B = [e
     c d]        y]         f]


where A is the /coefficient matrix/
X is the /solution vector/
B is the /constant vector/

above linear equation system can be now written as:
  AX=B ;; x A⁻¹ <- inverse
and solving for X, ideally the single point where two lines intersect:
  X=A⁻¹*x
From this we can see that to get the solution we must know of A is actually _invertible_!

That's all well and good, but let's first consider how do we actually represent lines
using the above two linear equations?
Most people will be familiar with the slope-intercept notation:

y=xm+b  ;; where m is the slope and b the intercept (the point where the line crosses the y-axis)
        ;; note: the slope-intercept form _can't represent vertical lines!_ 

Real example m=0 and b=1
y=1 which is a line parallel parallel to the coordinate x axis and offset by one, and it can
be easily formulated into a function f(x)=1
But how do we use this representation with a form like:

  ax+by=e

First of what's up with the a, b and e? Well that's what you get when you algebraically manipulate
our fine slope-intercept form:

ax+by=c ;-ax
by = -ax + c  ;; and there is out "y=mx+b"

Now lets try to transform these slope-intercept form into general linear equations

(1) y=2     ->   0*x+1*y=2
(2) y=x ;;-x -> -1*x+1*y=0
which have a point in common at P(2,2), so let's see if we can find a solution using the matrix
form

 0*x+1*y=2 into matrix-form->  [ 0 1  x  [x    = [2
-1*x+1*y=0                      -1 1]     y]      0]
                                 A    *   X    =  B
So lets see if A is invertible, so we can solve for X:
|A| = | 0 1| = -1 invertible!!
      |-1 1|

well lets cheat here and get inverse matrix without a formal way:
A⁻¹= [1.0 -1.0
      1.0 0.0]

Now we plug this into the equation X =A⁻¹B
[1.0 -1.0 x [2  = [2
 1.0 0.0]    0]    2]   and indeed X = [2
                                        2]
Which is the intersecting point of y=2, y=x!!!

--------------------------------------------------------------------------------
An intuitive explanation of ax+by=c
the general form of a line above, the linear equation of two unknowns x and y
has, thank goodness, a visual and intuitive explanation!

Consider the line
y=x+2  and its linear equation representation:
-2+y=2

What we first want to do is get the matrix row out of there
   a b c into
 [-2 1 2]

the first two values a and b form a 2d-vector [-2 1] which is _perpendicular to the line we're
defining here_(!), while c tells us _where the perpendicular line intersect with the
line we're defining here_(!).
Now you should see the perpendicular line and how it raises to y=2 and the line
perpendicular to it is indeed y=x+2!!


delving into the third dimension..

the form can be extended to three unknown, but this is _not a 3d line_ it is a plane(!)

  Ax+By+Cz = d

here the vector [A,B,C] is perpendicular to the plane, like the normal of a surface. To
get a line, we need to intersect two planes, for which we need two equations:

aka two planes make a line. A plane can be represented by 3 non-collinear points, but we
the method above, where we take a vector (x,y,z) and the normal to of the plane (A,B,C)
and this also is enough to represent a plane. The vector (x,y,z) is on the plane if the
dot-product, which is implied being calculated in Ax+By+Cz, between (A,B,C) and it
equals 0 (d = 0). So now if we plug in values for x,y,z we can test if they're on the plane
if the equation equals 0. Now to get a line we want to plug in the same values into another
plane definition of the form Ax+By+Cz=d if they're also equal d=0 for the same values
then we'd have found a point in common on both planes. Good, good. Now to get a line of
points in common is also the definition of a plane.

(1) A1x+B1y+C1z=d1
(2) A2x+B2y+C2z=d2    How do we solve this _linear equation system_? Well forget that for
                      now. We have the normals for both planes readily available all
we need to do is just calculate the cross-product of the two [A1..]x[A2..] and we get
a vector that is collinear with our line!! Now we just need to put this line somewhere,
a point to center it on. This can be calculated by solving for a point they have in common.
A solution involves just taking in one of the summands and set its coefficient x,y or z
to 0, and solve the equation such that we get a point to center our cross-product line
on it.

See below why a point and the collinear line suffices for the representation.

--------------------------------------------------------------------------------
Geometric representation of lines with vectors
   
v1=[x,y,z] v2[x2,y2,z2]

the set of lines, L, would be represented as: L={v1+s*(v1-v2)| s ∈ R}
If we write this in _parametric form_:
  x = v1.x e+t*(v1.x-v2.x)
and analogous for the other coordinates

--------------------------------------------------------------------------------
Now let's see what this would look like for a linear equation with infinite solutions
(parallel + coinciding lines)

(1)y=2, (2)y=2 both's general linear equation is: 0*x+1*y=2

|0 1 = determinant is 0!
 0 1|  no plane is span!

So we can't even calculate the inverse for the X=A⁻¹*B. Let's solve these equations the
old fashioned way:

(1) 0*x+1*y=2 => y=2
(2)     "        y=2
(1)-(2): 0=0 and indeed, "infinite solutions"

The book derives the AX=B form by stating that
x = (de-bf)/(ad-bc)
where a,b,c,d are the elements of the A matrix and e="c" of (1) and f="c" of (2)
so we can substitute
x = (1*2-1*2)/(0*1-1*0)
x =  2-2     / 0  ;;<- boom division by zero!! In this case, geometrically,
                  ;;   the lines have infinite common points, or every point of one
		  ;;   line is common with the other line
                 
--------------------------------------------------------------------------------
Now the two parallel, non coinciding lines
(1) y=2 (2) y=3

(1) 0*x+1*y=2
(2) 0*x+1*y=3
and we can already see nothing changed on the left side in this particular example
but the determinant is 0.0 and again the above x solution is going to be a division
by 0.

conclusion: If the determinant, of the coefficient matrix, is not zero, then the linear
equation system has one unique point in common. This also means that the collision test of
_whether_ a collision has occurred is already solved when det(matrix) != 0 is true!

--------------------------------------------------------------------------------
Cramer's rule,
solves linear equation system of arbitrary size (for example also 3 x 3). It tells us if a
collision between objects represented by those equations of the same arbitrary dimensions
has occurred and tells us where.

The If: when the coefficient matrix determinant is non-zero

The where:

given an linear equation system,
a1x+b1x+c1x=d1
a2x+b2x+c2x=d2
a3x+b3x+c3x=d3



represented as a matrix, and in determinant form:

|a1 a2 a3| 
|b1 b2 b3|   = our coefficient matrix d,
|c1 c2 c3|

In the above examples we solved this with X=A⁻¹*B where B would be the vector
[d1,d2,d3]. We need to keep this in mind as for performance reasons. But Cramer's rule
allows us to get the individual elements of the unique point P(x,y,z), the point that
solves all equations, by providing equations that solve for each of the points.

Getting the points is just replacing the column in order of the component one wishes to
compute by the constant vector, compute the determinant from that and have it be divided
by the coefficient matrix' determinant.

Performance notes: for systems with more than 3, 4 equations /Cramer's rule/'s
amount of work increases drastically. The author recommends using the
/Gaussian elimination algorithm/(!!) But for smaller system of linear equations
the rule is recommended as it is easy to apply, also the ability to calculate
individual components of the unique point is stressed.
Just to compute the solution of 4 linear equations 360 multiplications, 4 divisions
and 115 additions are necessary.

--------------------------------------------------------------------------------
cross-product:

[4      [12     \       ;; where i is repeating {1,2,3}      /
 3   x    6      \                                          /
 2]       9]      *---> v1.i * v2.i+1  - v2.i * v1.i+1 <---*
                      = v1⨯v2.i+2

(- 27 12)  15
(- 24 36) -12
(- 24 36) -12

--------------------------------------------------------------------------------
Inverse, defined as:

A⁻¹= 1/det(A) ;; <- again the determinant being the denominator cannot be zero

--------------------------------------------------------------------------------
Determinant predicate,
like in cl predicate implies that we will test for something, as seen with the sphere
formula or if something is solvable like with inverting or solutions to linear
equation systems with Cramer's rule.
_Most geometrical tests can be cast in determinant form_(!) Because of this determinants
have been heavily studied including performance computing solution.

sign of the determinant (+,-),
plays an important role and can imply:
- orientation (ORIENT2D function below)
- sidedness (also ORIENT2D)
- inclusion of points (sphere)

performance correctness trickery:
- look out floating point error amounting from arithmetic operations that are part of the
  determinant calculation
- going straight for the determinant is performance-wise usually a bad idea, better:
  divide-and-conquer or subexpression elimination like in Cramer's rule

--------------------------------------------------------------------------------
orient2d(A,B,C), 
takes 2d-points A,B,C ; is an example of a predicate determinant for sidedness AND
orientation of points

calculates a determinant predicate:

                  |ax ay 1|
orient2d(A,B,C) = |bx by 1| = |ax - cx   ay - cy|
                  |cx cy 1|   |bx - cx   by - cy|

Which has _many_ properties:
		  
orient2d(A,B,C) > 0, C lies to the left of the directed line AB. And as a plus this
also means the triangle is oriented counterclockwise (ccw).

orient2d(A,B,C) < 0, C right of the line; triangle CW.

orient2d(A,B,C) = 0, points are all collinear!

the actual value returned corresponds to twice the signed area of the triangle (negative
if triangle is CW)!

--------------------------------------------------------------------------------
orient3d(A,B,C,D),
here we deal with supporting plane of the triangle ABC.
"above the plane" means that from D view the triangle is counterclockwise
< 0 :  D is above the ABC plane
> 0 : below
= 0 : coplanar

It is the calculation of the 4d-matrix determinant where the first three columns are
Point's x,y,z values and the 4th being 1.

|a.x a.y a.z 1|
..
|d.x d.y d.z 1|

The value returned is _six_ times the signed are of the tetrahedron formed by A,B,C,D

--------------------------------------------------------------------------------
incircle2D(A,B,C,D),
the points of the CCW triangle ABC are on a circle, then
> 0 means that D lies inside the circle 
< 0 D lies outside
= 0 the four points are cocircular!
If the triangle is CW (same as orient2d(A,B,C) < 0) then the results are reversed

computation:

incircle2d(a,b,c,d) = |a.x ay a².x+ a².y 1|
                      |b.x by b².x+ b².y 1|
                      |c.x cy c².x+ c².y 1|
                      |d.x dy d².x+ d².y 1|

--------------------------------------------------------------------------------
insphere(A,B,C,D,E),
the tetrahedron's ABCD points lie and define the surface of the sphere, like
with incircle depending on orient3d(..) the opposite holds:

> 0 E lies inside the sphere
< 0 E outside
= 0 all points are /cospherical/

For INSPHERE and INCIRCLE2D the actual value has no special value.

--------------------------------------------------------------------------------
(2d) /pseudo cross product/, also: /pseudo dot-product/,
u⟂⋅v, where u⟂ (read: "u-perp" is the CCW perpendicular vector to u.
corresponds to: the signed area of the parallelogram formed by u⋅v. Positive
if v is CCW from u. Zero if they're collinear.

--------------------------------------------------------------------------------
Scalar Triple Product, or /triple scalar product/, or /box product/ [u v w],
because the operation:
  (u ⨯ v) ⋅ w
occurs a lot, it has its own name. Note that this is our determinant computation
of a 3x3 matrix from before(!) yielding the singed area of a parallelepiped,
also six times the size spanned u,v,w tetrahedron, using an additional point
to unite them at origin we recognize our Orient3d() function.

It also written as [u v w] and it holds true for any cyclic permutation

  (u⨯v)⋅w = (w⨯u)⋅v = (v⨯w)⋅u  !

note permute the vectors and then cycle the negations the following holds true

 -[uwv] =  -[vuw] =  -[wvu]

--------------------------------------------------------------------------------
/barycentric coordinates/,
we use reference points to represent points in the space enclosed by them - "parameterize
the space". Two points form a line, three a triangle. The representation allows then to
get at the points on the line or on the surface of the triangle.

A triangle ABC:
  A + u(B-A)*v(B-C)   ;;<- (u,v) are the barycentric coordinates, where if
                      ;; 0 <= u,v <= 1.0 the point is within the triangle

Imagine A to be the origin of a coordinate system of base vectors AB and BC.  Remember the
notes in arcsynthesis on the sin/cos brothers who when they work together give and take in
equal amounts: Imagine as "u" shrinks the AB vectors gets more directional and magnitude
influence from the BC vector and vice versa. See how it all grows out from the tip of the
A vector!
It should be easy now to see if a point is within the triangle, that is, whenever
0 <= u,v <= 1.0 is satisfied.

Uses of barycentric coordinates:
Given the BARYCENTRIC implementation we can test if a point P is inside a triangle ABC.
Also the use by OpenGL should be intuitive by now. All the things encountered in the
Arcsynthesis-study, or simply put whenever we needed the interpolated values from a
triangle like colors (Gouraud shading), normals (Phong shading) or texture texels/pixels
(texture mapping)!

--------------------------------------------------------------------------------
Lines, Rays, and Segments

consider the definition of a /line/ of two _distinct_ points

L(t) = (1-t)*A+t*B

the peculiar (1-t) indicates that if t=1 the point on the line is B:
= (1-1)*A+1*B
=   0  *A+B
= B

conversely if t = 0 the point on the line is A
= (1-0)*A+0*B
=   1  *A
= A

it should be easy to see how t grows, shrinks and inverts the A+B vector in effect
pointing at any point on the line.


/line segment/, or just /segment/,
is the portion connecting A and B which when we look at the t=0 and t=1 value computed
above is simply defined as the range
  0 <= t <= 1 !

  
/directed segment/,
if the endpoints A and B of the segment are given with a definite order in mind


/ray/,
is a /half-infinite/ line!! This is quite a helpful visual explanation because t is
only limited by
  t >= 0
i.e. only one direction is limited, namely below 0, while the other is infinite -
half-infinite!!


The parametric equation of the line L(t) above can be rearranged into:
= (1-t)*A+t*B
=  A-tA+t*B
=  A+tB-tA  //*swap*
=  A+t(B-A)

  hence we get:
                         L(t) = A + t*v    (where v = B - A)
                             ^^^^      
_Rays are usually defined in this form above_!!


parametric equation of the line,
the two representation of a line shown above:
  L(t) = (1-t)*A+t*B
and
  L(t) = A + t*v

--------------------------------------------------------------------------------
Planes,
- represented by three non-collinear points: 

P(u,v) = A + u(B-A)+v(C-A)            (parametric representation)

remember the non-intuitive order of vector subtraction "B-A" indicates a vector
from _A_ to _B_, then, the above equation represents a flow from A into B and C
spanning a plane, and triangle with parametrized B-A C-A sides by "u" and "v".


/in front the plane/,
given a triangle ABC the convention is that when viewed /in front/ 
- the _points are CCW_ and
- the _normal points towards the viewer

conversely /behind/ the plane is the opposite
In OpenGL we use this property to cull triangles from the rendering process. This
is efficient as only one side of a triangle can be viewed and we can form meshes
with clock-wise similar triangle arrangement on their surface so the rendering
engine can safely cull the invisible insides from the computation.


another representation is the:
- normal and point on the plane representation

I like the idea of it, it seems like the simplest intuitive representation of a plane
to me.

Anyway we can implicitly define a plane using a normal and a point on the plane.
P = Point on the plane
n = normal of the plane
X = some point on the plane

the following holds true:

n⋅(X-P)=0

see how the vector points from P to X would be perpendicular to the normal if the
dot product is indeed 0?
Given this representation we can easily test the containment of the point X in the plane
(namely if the equation is true)!

But wait there is more! The dot product is a /linear/ operation which is a fancy way
to say it can be distributed across addition and subtraction hence:

n⋅(X-P)=0
n⋅X - n⋅P = 0
n⋅X = n⋅P  

and finally if we use d = n⋅X
n⋅P = d                         // yeah, it's transposed

which is called the /constant-normal form/!

This form has some astonishing properties. When n is unit, |d| equals the distance of
the plane from the origin, I think with the help of transformation we can hence with get
the distance from any point.

But we're not done yet with equation transformations, the /constant-normal form/ can
be rewritten by marrying n's components with X's without adding:

ax+by+cz-d=0 ;; the ax.. cz part are the components of the normal, n = (a,b,c) and "P",
             ;; X = (x,y,z) vector, "d" is ;; just moved back to the left-hand side of
	     ;; the equation.

This will be our plane form to go.

/normalized/ plane,
when the n normal in the above equation is of unit-length, we say the plane equation is
/normalized/!
It has the advantage of simplifying most operation involving the plane. A very useful
operation using this form is solving for a point: the result will be the signed distance
of the point to the plane! Again with the ORIENT3D-like properties(!):
 negative result =  point is behind the plane
               0 = coplanar
        positive = in front


/dihedral angle/,
angle between two intersection planes

/hyperplane/,
This is how we refer to a plane in an arbitrary dimension. They always have one less dimension
than the space they reside in. From this, strangely, follows that a hyperplane in 2d space
is a line! And a hyperplane in 3d space is an ordinary plane.

/halfspace/,
furthermore, any hyperplane divides the space it is in into two sets of infinite points called
/halfspaces/!
If the points on the dividing hyperplane are included in the two halfspace sets then the
halfspace is:

/closed/ otherwise

/open/!

/positive halfspace/,
the side to which the normal of the hyperplane points is the positive halfspace, otherwise
it is the

/negative halfspace/.

/halfplane/,
when a hyperplane divides a 2d space (a line in 2d space) the resulting halfspaces are also
referred to as _halfplanes_. Indeed, because they are ordinary 2d-planes.

/convex point set/,
a set of points which when any two of them would form a segment, they wouldn't be contained
in the same set.

/convex hull/
CH(<set>), the smallest convex point set fully containing the <set>

/affine hull/
lowest dimensional hyperplane containing the convex hull of a given set.

---------------------------------------------------------------------------------
Testing polygonal convexity,
since collision systems dealing with convex polygons are rather faster, we want to
test convexity. Triangles are special in that they're the only polygon which
is always convex! But it is usually more efficient to perform collision tests against
a single concave n-gon then against multiple proxy-triangles mimicking the n-gons shape.
We want hence to test all faces for convexity either at run-time, or at "tool-time"
--when the artists sculpture the objects in question.

quads convexity,
a special case, usually supported next to triangles.
(!) We assume the four points that make up the quadrilateral are coplanar!
If its diagonals intersect = convex,
If not, the quad is concave or self-intersecting
But we won't actually compute the intersecting point but rather use the property of the
intersection point of the two segments:
each diagonal points are on the opposite side of the diagonal they intersect. This in
turn makes the triangles in play BDA BDC have opposite winding orders, as well as
ACD ACB. The opposite winding order can be computed with the cross-product and the
resulting normals can be dot-product compared:
if the dot-product is negative the normals point in different direction and hence
stem from opposite winding-order triangles (remember we assumed from the very beginning
that all points are coplanar!)

Hence the quad is convex if:
(BD x BA)⋅(BD x BC) < 0 and
(AC x AD)⋅(AC x AB) < 0


n-gon convexity,
there is an easy intuitive solution: test for every edge if all the vertices lie on
the same side of the edge, plus, for robustness, a check for coinciding vertices.
With complexity O(n²) it becomes an issue for larger inputs. Heuristics exists with
more favorable complexity but also with false positives (called a non-convex polygon
convex and vice-versa).
One such test is to test if the interior angle is < 180-degrees. Which fails for
self-inserting polygons like the pentagram. With the assumption that self-insertion
is not given it has linear complexity.
Another algorithm tests the directional change from edge to edge, either we
steer in one direction consistently or we have a concave n-gon. It fails for
collinear points.
But if we combine the two tests we get a rather robust and efficient algorithm!

--------------------------------------------------------------------------------
Polyhedra,
"3d polygons". Made out of some flat polygons glued together at their edges (max and min 2
polygons per edge). Polyhedra have vertices, edges, faces.
Can be convex: if the point set they enclose is convex - i.e. the line of every two points
contains points from the interior of the polyhedron. The boundary points of the polyhedron
are part of the convex hull of the polyhedron and determine if the polyhedron is convex or
not.
Speaking of which, the polyhedron boundary divides space into two disjoint regions:
the /interior/ and the /exterior/.

/polytope/,
a convex (bounded, i.e. faces have no indefinite extend) polyhedron

/simplex/,
like the affine hull of a set of points the simplex is the convex hull of a given set. But
unlike the affine hull it doesn't aim for the minimal dimension possible, instead its
dimension is directly linked to the magnitude of the point set:
d-points are contained by a (d-1)-simplex, e.g. 2 points are contained by a 1-dimensional
convex hull -- the 1-simplex: "a line".
3-points are contained by a 2-simplex.
Conversly this means that adding points to the set in question, or taking points from
the set changes the dimensional of the simplex, and the shape of the simplex.
Finally given 5-points who would have 3d affine hull would have a 4-simplex.


/supporting point/ of a _convex_ set,
given a direction and a convex set, it is the point furthest from this general direction.
The math definition is rather helpful:
given a direction d and a point P:  d⋅P=max{d⋅V : V ∈ <convex-set>};
note that:
(dot-product (vec3 1.0) (vec3 1.0)) ==> 1.0 ;; dot-product increases with "longer" vectors
(dot-product (vec3 1.0) (vec3 2.0)) ==> 2.0 ;; don't just think of it as the cos-angle!

It is also sometimes called /extreme point/ but it isn't necessarily unique: Imagine a
convex polygon with the furthest point lying on a edge 90-degrees to the given direction,
then all the points on the edge would be supporting points.
If a supporting point happens to be vector, it is aptly called a /supporting vertex/.

A supporting point can be understood as the first point to make contact with a force comming
from the opposite direction (think: analogous to TOI - space of first impact).

/support, mapping/ or /support function/,
a function that maps a direction "d" into a supporting of of a convex set C
f(d)->C
Computing the supporting point for a polytope is a trivial O(n) algorithm that greedily finds
ever more extremer points, while iterating through the vertices. It should be easy to picture
why testing only the vertices suffices.

/Supporting plane/,
plane _through_ the /supporting point/ with the _direction as the normal_. Now it makes
much more sense to think of "space of first impact". We will probably use this to expose
the representation of an n-gon needed at the point of interest --the point supporting the
polygon best against impact-- for a collision test!

/separating plane/,
an open _halfplane_ separating _two_ _convex_ sets of points such that in one direction
(positive/negative) of plane is one and on the other side the other convex point set.


/surjection/,
each element of the codomain is mapped to at least once (imagine a bunch of soldiers
totally outnumbering the enemy and having every single one of them at gun point with
one or more soldiers per enemy unit. The enemy must SURrender.)

/injection/,
each element of the codomain is mapped to at most once. (after an INJECTION the virus
contaminates the body, but it never attacks single cells in pairs.)

/bijection/,
function is both surjective and injective

/homeomorphic/,
a function that maps two topological spaces is called homeomorphism if:
- it is a bijection
- it is continuous
- its inverse is continuous
Intuitively this means that you can morph one shape into the other without "non-linear"
transformations (imagine kneading dough, it is all homeomorphic until somebody pulls it
apart)

/2-manifold/, also called /surface/,
the surface of a polyhedron is often referred to as a /2-manifold/! "This topological term
implies that the neighborhood of each point of the surface is (...) [homeomorphic] to a
disk.
From the perspective of a being living on the 2-manifold, on our polyhedron surface,
everything around it would look flat. Furthemore wherever it would go everything would
look flat and, this is important, it would never run into a wall (unbounded)!
This is very similar to us living on earth: it looks flat most the time, and whereever we
go we won't find a an edge to fall of the earth (no boundry). Another thing is also true:
when we walk long enough in one direction, we end up in the same place.


/Euler formula/,
neat relation of vertices (v), faces (f) and edges (e) of a polyhedron etched into an
equation:
 v + f - e = 2


/convex polyhedron/,
for all the faces, the points of the polyhedron lie on the same side of the face.

complexity:
O(n²):
when we test N faces against N points.

O(n):
Compute the centroid of a face (the mean of all position vectors), then take a neighboring
face and compute its supporting plane. Now test if the cetroid is behind the supporting
plane. If the test fails = concave, else = convex!
I.e. we only need to test linearly through the faces

--------------------------------------------------------------------------------
/Computing Convex Hulls/,
the convex hull can be used as a tight bounding volume -- when precise collision
test are needed, instead of a more inacurrate but more efficient proxy shape (like
a sphere). The following algorithms compute them:

Andrew's algorithm,
robust and easy to implement _2D_ convex hull constructing algorithm.
The points are sorted lexicographically by their coordinates (x-coordinate
left-to-right). We start with the upper part of the hull and add the point A to the
chain. We form an edge with B. With every subsequent point we test on which side it lies
in respect to the last edge formed (AB). If it is on the right we add it to the chain, if
it is on the left we remove the last point from the chain (B) and replace it with the new
point (C). We fair analogously with the lower chain of the convex hull.

Quickhull algorithm,
works in 2d and 3d. 2d explanation: first we use the 4 cardinal directions and choose
extreme points of each drawing a quadrilateral. Then we recursively add points furthest
from each of the quadrilaterals edges until every point is enclosed by the convex hull.
This can be extended to 3D (also even higher dimensions) by simply finding 6-extremepoints
for the 3 perpendicular lines times its 2 directions. Now instead of edges we deal with
polygon faces, hence adding points that are furthest from a given face adds 2 or more
faces.
/Qhull/ by "Brad Barber" is an implementation of the algorithm available for download.

--------------------------------------------------------------------------------
/Voronoi Regions/, also /Voronoi Cell/ or /Dirichlet Function/,
given a set S of points P: the Voronoi region _of a particular point in S_ is the
set of points closest to P THAN any OTHER point in S (P1, P2, ..., Pn).
That means that if you take any point in the voronoi region, the distance to it
from the Point P (the point the voronoi region is definined by) is always shorter
than any other point Ps in S.
/Voronoi feature regions/,
will be used interchangeably with the above. It is important to note that a /feature/ of P
(in the set S above) can be not only a point (vertex) but also an edge and face.

/Voronoi diagrams/,
these are the set of points who are equidistant to two or more points. Visually
speaking the boarders of the Voronoi regions are voronoi diagrams, if they're
surrounded by voronoi regions for other ponits and partially if they're on the
fringe of voronoi region field (Since it only partially boarders to other regions
and can hence not have _atleast two_ points equidistantly to it.

--------------------------------------------------------------------------------
/Minkowski Sum/,
A, B ⊂ V where A and B are subelements of the Vector space V. Then the minkowski
sum A+B or

  A ⊕ B := {a+b : a ∈ A, b ∈ B}

a + b, is the _vector sum_ of the positions vectors a and b

This one has a nice visual interpretation: Imagine that A and B points each
form a figure (a polygon when 2d). Now think of the concept of translation
as in graphics transformation: if you use each point of B as origin for the
translation of points A you get a bunch of new points. The swept region
by the translation is the Minkowski sum!


/Minkowski differece/,
analogously:

  A ⊖ B := {a+b : a ∈ A, b ∈ B}

Geometrically we add A to -B, i.e. A ⊕ -B = A ⊖ B. Visually we sweep the
region of a B relfected about the origin (-B) with the figure A.


Because both the Minkowski difference can be interpreted as a sum they are often
simply referred to as Minkowski sum.


Minkowski sum useful properties:

The Minkowski sum of two convex polygons and polyhedra is also a convex polygon or
polyhedra (!), which should be easy to imagine using the swept region interpretation.

Now for collision detection the visual interpretation makes extra sense as we basically
build a proxy bounding volume about an object in regard to the other. This approach is
useful when dealing with complex objects. The object we want to test for gets minkowski
summed with another thereby we "grow" them, at the same time we "shrink" the other
because:
if A ⊖ B contains the origin => the objects collide!!!
Simply because:
Given a point (1,1) in both objects:
(1,1) - (1,1) => (0.0)  <- origin!

But that's not all! We can _compute the minimum distance between A and B_(!)_ which is
simply computing the distance between (A ⊖ B) and the origin!!


TCSO,
Minkowski difference is also referred to as TCSO: /translational configuration space
obstacle/. "Queries on TCSO are said to be performed in configuration space"

--------------------------------------------------------------------------------
--------------------------------------------------------------------------------

-- BOUNDING VOLUMES --

Bounding volumes (BV) are usually specified in the local model space which may also be the
world space.  Regardless for collision to work the test must of course be performed in a
common coordinate system.

local space transformations,
a solution is to transform one of the BV into the local space of another.
Advantage:
- a world space is big, rounding imprecisions can occur
- tighter BV (thing AABB, at least the local space of one rectangle will have a convex hull
  as the BV

world space transformation,
This is important: We can _cache the updated BV (!) in world space _for the duration of
the time step_!! This makes sense in world space because repeated overlap test with
multiple objects can all use the same cached future BV's. In contrast the local space
transformation caching doesn't make sense for muliple overlap tests!
Disadvantage:
- double storage usage


/nonaligned, or, freely, oriented BVs/,
spheres and CH (convex hulls) can be transformed into any coordinate system. In contrast

/aligned BVs/,
such as AABBs imagine you AABB rectangle (CH) by 45-degree. You need to realign them
after rotations. There are four common strategies:
- fixed-size loose AABB that regardless of rotation always encloses the object (you can
  imagine it being particuallarly large to fit all the orientations of the object).
  Given a center point and the furthest vertex from it we shape a sphere(!), then we draw
  a boundign box around it and we get the fixed-size AABB.
  It is the simplest solution at the expense of accuracy due to being a loose BV.
  The advantage: rotations can be ignored, all we have to do is translate the BV upon
  object movement.
  Finally: The sphere formed from the center point (the pivot) and the furthest point
  forms a better BV. The collision test is also efficient and it clearly is a tighter,
  more accurate BV!
the other three involve dynamically resizing the AABB:
- find the extreme points in all directions. For a triangle it is simply projecting the
  points along the (cardinal) direction vectors and finding the outer most

--------------------------------------------------------------------------------

Spheres,
center-point + radius = sphere representation.

Sphere collision test,
is just the distance between the center-points being smaller then the sum of the
radii. (For efficiency we square the distance using the Dot- Product. As computing the
distance involves pythagoras a²+b²=c² √c² = c, so we simply use c² in its square form
saving the expensive square root computation. Squaring the distance with the dot-product
is very efficient on current hardware.


"Ritter's Sphere",
computes an approximative sphere BV from a point set. First we find extreme points
in the 6 3d-axis direction, then we use the furthest two along a common axis. Then
we pivot the center point at its middle and cast a sphere around it, encompassing
the extreme points.


direction of maximum spread,
instead of using the AABB of a point cloud and find the most distant points along each of
its axis to calculate the approximate sphere (Ritter Sphere), we can use the power of
statistics.  By finding the direction of maximum spread: that is that the ponts projected
on this directional-axis would have the maximum spread as opposed to some other axis.  The
points on the two ends of this projection would be used to define the surface of the
sphere, its mean the center.

How do we find the direction of maximum spread? 

Variance & standard deviation,
The mean is a center of some set of numbers that comperatively for each number
shows how much it deviates from all other numbers, where bigger means it usually
bigger than most numbers...
Usually...?

Imagine you get 80 out of 100 points on the test, you also find out that the average score
is 70, so you feel pretty content being above average. But the tricky part is: there was a
person missing and he got a grade 0 why two others got a score of 100. Now the average is
70 but most people have a perfect score!
This is where variance can come in handy, especially standard deviation:
standard-deviation: 41.23 which tells us, using the mean=70 that values in the interval
70±41.23 are pretty average. Which means that your score and the two aces are no big deal
but that one guy missing is a standard deviation indeed.

Finally _Variance_ is the "mean of squares of the deviation from the mean", why do we
square the values and then harvest the sqrt to get the standard deviation?
The answer is if the number set invoves negative values, they can nullify their
positive brethren, this is where squaring moves everyone to the positive realm (* -1 -1) ==> 1
and distributes them meaningfully.
