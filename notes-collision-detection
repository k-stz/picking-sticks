Notes on: "real-time collision detection" by Christer Ericson

explicit representation,
objects represented by polygons, through their points, edges and faces

implicit representation,
geometrical objects such as spheres, cones, cylinders, tori (pl. torus), etc. which
can't be explicitly represented but implicitly through a mathematical expression:

implicit representation are usually represented as a mapping from 3d-space to 1d-real-numbers
like so:

[first, new, cool, math thing!]

f:R^3 -> R, where points given:
f(x,y,z) // our R^3
will evaluate into three regions zero, positive and negative:

f(x,y,z) = 0 // boundary of the object
f(x,y,z) > 0 // exterior of the object
f(x,y,z) < 0 // interior of the object

awesome!

Example for a sphere:
x^2 + y^2 + z^2 <= r^2    // works!

This is like a double Pythagorean theorem on the left side the resulting
distance is, if equal r^2, a point _on_ the sphere

(defun sphere-collision-p (x y z sphere-radius)
  "Returns true if the point P(x,y,z) lies within or touches the sphere of radius:
   sphere-radius."
  (<= (apply #'+
	     (mapcar #'(lambda (x) (expt x 2))
		     (list x y z)))
      (expt sphere-radius 2)))

Through understanding that this is just a double-pythagoras we can formulate a collision
test for a 2d-circle easily:
x^2 + y^2 <= r^2  (awesome number two!)

An object boundary defined in such a way is called an /implicit surface/!

Advantages:
- quick rejection culling (is it in the interior = CULL it out!)
- fast intersection test: well simply feed a point into the f(x,y,z) function and you know
  if it is inside or touching the boundary: f(x,y,z) =< 0!

half-space,
we cut space in two along an infinite plane. The resulting two spaces left and right of
the plane are infinitely large but still just half-spaces of a bigger infinity!

convex objects (polygon),
any two points within this object can connect in a straight line without leaving the region of
the object. We need this word because collision tests are much easier with such objects!
Foreshadowing: we will probably try to break concave polygons into smaller convex objects?

concave polygon object,
opposite of convex object. Contains an interior angle greater than 180-degrees.

half-space intersection representation (convex objects only),
another way to represent polygonal objects, here we cut away some space with planes and use
the resulting half-space intersections to represent an object. An example would be to represent
a rectangle in 2d by using two x-axis values to cut away the two sides - resulting in an
infinitely long rectangle along the y axis- then provide two y-axis to encompass a finitely
big rectangle! (see texatl.cl:space for this kind of rectangle representation!)
This is reminiscent of the aabb test!

--------------------------------------------------------------------------------
We won't usually use the same geometry for collision as for rendering
Pros:
- it is usually too complex, and collision test don't need to be that precise
- the usual triangle representation of objects are harder to query for collision
  then other shapes like spheres, for example
- rendering data may be organized in quite a different way than is useful for
  collision. Collision tests probably want spatial hierarchical relationships
  whereas rendering data will create topologies for adjacent vertices at best
  and material attributes, color and texture coordinates at worst (useless for
  collision tests!)
- organizing for collision tests makes the data smaller hence more efficient to
  use and coherent in memory and useful for further extension and tests tailored
  for collision
- sometimes you want to explicitly differ, for example the plane representing
  snow might be intentionally lowered in the proxy geometry to allow knee-deep
  movement in the snow. Instead for exclusion tests, just use a different geometry
  to begin with.
- even if rendering data is not needed because an object is not visible it is
  still needed for collision. Just imagine walking behind a wall, where the
  camera sees the wall it will usually cull out everything behind it, you should
  however still follow the law's of physics even when nobody is around to see it!

Cons:
- Data duplication. Could be solved by inferring the proxy geometry from the rendering
  geometry (through linearization caching)!!
- time to build the tools that generate the geometry or more time for the designer
  to create his models
- visual consistency may suffer: objects may float above others, or fully intersect
  (surely you seen the animations which simply move through anything when parts
   of it move outside the hitbox)
- interdependency problems: if the rendering geometry changes, the collision geometry
  might also have to change, and how is this process automated. Which of the two
  geometry comes first?

we use, hence, another geometry instead.. as a proxy:

proxy geometry,
the collision test geometry for objects. This usually reduces the complex rendering
geometry by encapsulating a object of interest into a:

bounding volume,
a simpler geometrical object like a box or a sphere representing the object in a
simpler way than the actual rendering geometry for the sake of a simpler easier
collision test!

--------------------------------------------------------------------------------
Types of queries:

interference detection or intersection testing,
yes/no did the objects collide, do they intersect?
Easy to implement, commonly used

intersection finding - WHERE did they intersect?
One contact point may sometimes suffice other times, as in rigid body simulations,
we need to detect a whole set of contact points /the contact manifold/!
Calculating the contact manifold may be difficult, and approximations may be used -
a common practice in games.

collision properties,
special collision queries, taking surface properties into account like slipperiness of a
road and climbability of a wall

penetration depth,
how deep are two objects overlapping. The problem here is what start and end point to
use to measure this. A solution to this can be the shortest movement vector needed
to separate the already interpenetrated objects. Its computation is also a difficult
problem.

/separation distance/, has a beautiful definition:
"The separation distance between two disjoint objects A and B is defined as _the minimum
of the distances between points in A and points in B_"!
Needed the predict the time (WHEN) two objects will collide. Also it poses another
problem of finding the

closest point,
in both the disjoint object A and B to each other. There can be infinite cloest points!

ETA, /estimated time of arrival/, or, more dramatic, TOI, /time of impact/

--------------------------------------------------------------------------------
pairwise collision tests for n objects, take O(n^2) time - every objects with
every other object n x n!
But we can use a divide and conquer approach:

broad phase or n-<body> processing,
use a rule to decide which objects /may/ collide. For example divide a scene into
square regions, and whenever an object is inside a given region it is mapped to
some square-region data structure. This data structure can then exclude all the
objects that are sole inhabitants of a region from any collision test. The
other objects under go the

narrow phase (pair processing),
perform all the pairwise test on the unconquered parts (those objects
sharing the same square region!).

--------------------------------------------------------------------------------
Sequential vs simultaneous motion,
for both we break down movement to the unit of _time step_(!). Simultaneous motion
is more realistic but more expensive to compute:
(1) determine earliest time of contact
(2) move all objects for the extend of the no-contact-time
(3) resolve the collision
(4) repeat for the extend of the time step
A problem occurs when the object moves along a surface, then a collision resolution
would take place all the time and the simulation would only advance by a small
fraction.

While broad phase grouping can help divide and conquer the problem.  An alternative
simultaneous motion solution: advance the objects by a fixed small time step, when an
interpenetration of objects during a time step occurs restore the state before the
time step. This is also expensive.


The accuracy of simultaneous motion should be reserved for rigid-body simulations.
For most program sequential motion suffices (including games).

Sequential motion accuracy problems:
(1) Two objects during motion, in a sequential approach one object might move past the
collision point, then the other object also moves past it -> no collision occurs. (2) One
object is just behind another object, when it moves first it will catch up but if both
move at the same speed this catching up wouldn't make sense.  Both these issues _can be
solved by just making the time step smaller_!
In games the small time step is easily provided by the _high frame rate_(!)

sequential motion benefits include easy penetration resolution: just undo the single
motion during the small time step, this is to be contrasted by the simultaneous where we
have to undo motion during a time step for _all_ the objects (possibly minus some
broad phase excluded ones)!

--------------------------------------------------------------------------------
Discrete vs continuous motion

Discrete motion - static collision detection,
Collision detection is measured at a specific time, and the objects are treated as if
they're frozen in said time (This is much cheaper than dynamic collision detection below)
The object effectively teleports from time step to time step, the collision test is hence
less precise. On the note of teleportation: the time step teleportation distance has to be
smaller than the objects spatial extend or else the object might teleport through other
objects with no collision being detected, this is aptly called /tunneling/!!!

Excursion on tunneling:
To have more fun memorizing this, in quantum mechanics teleportation is an everyday thing
performed by particles all the time, called quantum tunneling. According to QM electrons
perform quantum leaps to move between energy states in atom, as the wave(wave-particle
duality) travels around the nuclei of the atom it reinforces itself on each iteration
thereby maintaining its energy level.

Another exciting thing: scientist were able to quantum tunnel a particle over the distance
of 9 meters. But this doesn't mean teleportation of humans is gonna any time happen soon,
when prompted for this an expert said that they had to get the particle in a particular
state before performing this, and the particles in human bodies just can't be manipulated
into such a state anytime soon.
/Excursion

dynamic collision detection,
we detect the collision by considering the continuous motion - meaning over a given time
interval or between start and end of a time step. This is both more accurate and more
expensive. More accurate because we can usually tell the exact time and point(s) of first
contact.

swept volume,
in continuous motion, this is what we refer to the space carved out by an object as it
moves for the extend of a single time step! This can be used for the broad phase: only if
the two objects' swept volumes do intersect do they become member of the same narrow phase
group. Of course even then collision is not guaranteed (should be easy to imagine
why). But imagine an object moving in a non-trivial path during the time step, the swept
volume can then be hard to compute and work with! Fortunately such accuracy is rarely
needed, and can be reduced for simplification to a linear motion.

speedbox,
in games the swept volume is usually simplified to an elongated box covering the objects
full range of motion (or some similarly simple proxy object).


relative motion,
what-is (high-level explanation):
subtracting two motion vectors hence one being stationary and the other moving towards it
for simpler intersection finding of the swept volumes. Consider v1 v2, for v1-v2, v2 is
the stationary "observer" where v1-v2 is the vector that moves towards it from v1 while,
again, v2 can't move. This explanation though is enough to work abstractly I want to
understand this at a visceral - intuitive level.
how-to (how to implement it explanation): (TODO: test this)
given two motion vectors v1 and v2 it is virtually always better to consider the relative
motion:
vp1 vp2 are position vectors, and v1 v2 are these vectors' motion (direction) vectors
the relative motion is the subtraction: v1-v2
The observer is the right-hand vector: vp2
it is immobile as vp1+(v1-v2) moves towards it (as far as the time step allows).
This is better because we now only have the swept volume carved out by vp1+(v1-v2)

Optimization,
is mainly achieved by the broad phase, putting objects into regions or region hierarchies that
may even be recursively extended when more objects govern a certain region (implementation
example: quadtree).

Robustness,
two problems: floating point imprecision and geometry nonsense through transformation. One
can give rise to the other, for example: space inverting on itself, through interpolation
off-by-one bug turning into extrapolation; or transformation order is violated and rotation
scaling distorts the geometry, or transforming normals on a non-uniform object (transpose
(inverse norm-matrix)), or systems prone to gimbal lock may have unexpected orientation at
certain stages of interpolation.  The collision system must be prepare and avoid those for..
robustness!


------------------------------------------------------------------------------
Math and geometry primer


transpose matrix of A: A^t

/skew symetric/ or /antisymetric/ matrix,
when A^t = -A. For example rotation about the x-axis, then the antisymetric matrix
would be a 180-degree turn around the x-axis

;; 90-degrees              ;; 180-degree of 90-degrees turn before
#<[1.0, 0.0, 0.0, 0.0]     #<[1.0, 0.0, 0.0, 0.0]
  [0.0, 0.0, -1.0, 0.0]	     [0.0, 0.0, 1.0, 0.0]
  [0.0, 1.0, 0.0, 0.0]	     [0.0, -1.0, 0.0, 0.0]
  [0.0, 0.0, 0.0, 1.0]>	     [0.0, 0.0, 0.0, 1.0]>

matrix addition,
pairwise addition of correspoing positions. Given matrix A=[a.ij] and B=[b.ij]
then the sum matrix C=[c.ij] = A+B is defined as the pairwise addition of elemens:
c.ij = a.ij + b.ij


lower trianglular matrix,
all the entries of the matrix above the main diagonal are zero

upper triangular matrix,
opposite of lower triangular matrix

row/column vector or row/column matrix,
an m x n (m by n) matrix is comprixed of smaller row/column matrices that make
up the eponymous rows/columns of the matrix. Also called the row/column vector.

singular (or noninvertible),
matrices that do _not_ have an inverse. For example: any matrix that has an all-zero
row.
Or more formally: if the determinant of matrix is 0, then the matrix is noninvertiable.


Algebraic identites involvoing matricies,
given scalar r and s and matrices A, B and C (of appropriate sizes required to perofrm
the operations) the following identities hold:

s(A+B) = s*A + s*B     ;; i.e. scalars can be factored out
A(s+r) = r*A + s*A     ;; matrices too

scalar*matrix multiplications are associative
r(s*A) = s(r*A) = (r*s)A

A(B+C) = AB + AC

(s*A)*B = s(A*B) = A(s*B)  ;; tricky

transposing,
(A+B)^t = A^t + B^t  ;; is distributive:
(s*A)^t = s*A^t      ;; doesn't mind scalars
(A*B)^t = B^t*A^t    ;; switches the order when dissolving parens! tricky!!


--------------------------------------------------------------------------------
Determinants det(A) or |A|
it is a _single number_ associated with the matrix and it tells us if the systems of
linear equations, that matrix is comprised of, is _solvable_. It is solvable when the
values is det(A) != 0. One of the things we want to solve is if the matrix is
_invertible_.

We will mainly need to calculate the determinant of 2 x 2 and 3 x 3 matrices

2 x 2: simply multiplying the values in the diagonals and substracting them.

|A| = |u1 u2| = u1*v2 - u2*v1   (imagine how you take out the values diagonally squish
      |v1 v2|                    them in your palm, finally you make them fight substracing
                                 each others troups)

before the awesome 3 x 2, check out the single element matrix:

|A| = |u1| = u1   which makes sense, since the identity matrix would be 1, and you can't
                  perform any transformations(arithmetic opereations) that could convince a 0
		  to be a one (remember det(A)=0 means, unsolvable as far as we're concerned)

finally, the cool one:

|A| = |u1 u1 u1| = u1(v2*w3-v3*w2) + u2*(v3*w1-v1*w3) + u3(v1*w2-v2*w1)
      |v1 v1 v1|
      |w1 w1 w1|



yeah.. never mind that, check this out this is an identity of the above:

|A| = |u1 u1 u1| = u Â· ( v x w)
      |v1 v1 v1|     ^     ^
      |w1 w1 w1|     |     cross-product
                     dot-product


		     
v x w returns a vector perpendicular to the plane that v and w span. Now the dot-product
of this perpendicular vector with u determines the cos angle between the two. Remember how
a determinant of (0.0, 1.0] means the the matrix equations are solvable? Well guess what,
this is the case when u and the plane-normal (from v x w) are perpendicular!  And this is
exactly the case if u lies in the same plane as v and w! Visually this is simply the case
when all three vectors share the same plane - or - if we _lose a degree of freedom (make
the gimbal mind mapping here!) (dimension) because one of the vectors collapses into the
plane of one of other two!!

note also how the cross-product for two linearly dependent vectors (parallel to each
other: either face the same direction or the opposite) is also 0! Which makes sense no
plane is being span, we lose a degree of freedom! Now a dot-product with vec(0,0,0) - a
point - is always 0 (we failed to span the initial plane, no 3rd dimension for you!).

Now we can see the broad pattern here: Whenever we lose one of our total number of
available dimension/degree of freedom the determinant is equal to 0! You should be able to
see it clearly now before your eye:

1 x 1 matrix: a single vector is 1 dimensional, if it equal ot 0, it no longer has extend in
any dimension geometrically and is just a point

2 x 2 matrix, we can quickly see if we have linearly dependent vectors or, if a row is 0,
a point and a vector and hence lacking the ability to span plane.

3 x 3 like described above

Finally, imagining the inverse matrix solution:
If we try to yield an identity matrix why is a determinant of 0 indicating that there won't
be a solution? What is the big deal with lacking a degree of freedom there? We can imagine
this using 1d-matrix that is just a vector whose determinant != 0 which is the case if
vector is "line". Now if were to transform an object using this 1d-matrix which is basically
a scalar, we could streth the object or shrink it in this one dimension. And because we
have the maginitutde with which we stretch the object - the scalar - we can transform it
back to an identity matrix:
scalar 3 ; /3
scalar 1 <- identity matrix

But what happens if the degree of freedom drops to zero? The vector (0) copletely annhiliates
the object (when being multiplied). Now how can we restore the "aspect" ratio of such an
object back to identity iw have lost the leverage, the dimension, to do so? We have no information
to make it back. This is why we can't inverse this such a matrix. (this might be related to
"covering map" and "homomorphism" and probably also with "gimbal lock").



crossproduct parallelogram,
the magnitude of the crossproduct is equal to the parallelogram with the vectors used for sides
but that's nothing:

signed area and determinant,
the determinant corresponds to the
- singed length (1d-matrix determinant)
- signed area of the parallelogram (2 x 2)
- signed volume of the parallelepiped of the (3 x 3)
- or the signed hyper-volume (!) of the n-dimensional hyper-parallelpiped

The parallelogram takes the two vector rows unites them at origin (0,0) and projects the paralleogram
from their respective tips. The parallelepiped is that parallelogram plus the columns from the
third vector on top of it. The hyper-parallelpiped would be a n-dimensional beast unfathomable for
us humans.

again note how we can't build any of these objects when a dimension is lacking: det(A)=0

--------------------------------------------------------------------------------
Determinant identities

|A| = |A^t|   ;; should be obvious now with the above signed area explanation

If B is obtained by interchaing the two rows or two columns of A, then |B| = -|A|
again obvious now, when we consider that v x w, w x v, an opposing vector
(use the _right-hand rule_!!)

This one is difficult to wrap my head around for now, makes sense for translating rotating
but not quite for scaling. Apparently multipying two matrices and yielding the determinant
produces the same parallelepiped as just multiplying the determinants of both matrices:

|A|*|B| = |A*B|


Another novelty:
You can objtain B from A by multiplying _a single column/row_ by a scalar k. This can't be
done with matrix with matrix multiplication. This method of matrix permutation is new to me
and must be noted!
Given this, the following is trivial:
|B| = s*|A|    ;; s being the scalar
We can easily imagine just growing a single vector is growing the parallelepiped by the same
factor as just multiplying the whole thing by the factor that grew the individual vector.
They're both just factors in a formular comprised of only mutliplictionsequation
So a cube of side 3*3*3 = 27, we grow a single vector 3*3*4 = 36, now growing the vector
required a factor of 4/3 because 4/3 * 3 = 4. And now the determinant muliplication:
(3*3*3)=27 * 4/3 = also 36

if A contains rows who are multiplies of colums or vice verse then |A|=0. Trivial, if they're
muliplies then they're lineraly dependent -> freedom loss -> no dimension to span a plane or
grow a parallelepiped -> volume,area,segment = 0 = determinant

is any column/row zero then ofc |A|=0, again dimensions drop -> ..

--------------------------------------------------------------------------------
Another way to calculate determinants:
_"use column and row operations"_(?) on the matrix to reduce it to a triangular matrix.
Now the product is then the product of the main diagonal entries!!